{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Section<a href=\"#Init-Section\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[1\\]:\n",
    "\n",
    
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow\n",
    "    tensorflow.__version__\n",
    "\n",
    "    # Initialize the random number generator\n",
    "    import random\n",
    "    random.seed(0)\n",
    "\n",
    "    # Ignore the warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    import io\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "\n",
    "In \\[3\\]:\n",
    "\n",
    "    import spacy\n",
    "\n",
    "In \\[4\\]:\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    print(stop_words)\n",
    "\n",
    "    [nltk_data] Downloading package punkt to /root/nltk_data...\n",
    "    [nltk_data]   Unzipping tokenizers/punkt.zip.\n",
    "    [nltk_data] Downloading package stopwords to /root/nltk_data...\n",
    "    [nltk_data]   Unzipping corpora/stopwords.zip.\n",
    "    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    Mounted at /content/drive/\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    import os\n",
    "    import sys\n",
    "    os.chdir('/content/drive/My Drive/AI Datasets/')\n",
    "\n",
    "# IMPORT LIBRARIES<a href=\"#IMPORT-LIBRARIES\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[7\\]:\n",
    "\n",
    "    import glob\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    import numpy as np\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.models import Sequential, load_model\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras import backend\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Embedding, LSTM, TimeDistributed, Flatten\n",
    "    from tensorflow.keras import metrics\n",
    "    import tensorflow as tf\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "    !pip install transformers==3.0.0\n",
    "\n",
    "    Collecting transformers==3.0.0\n",
    "      Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
    "         |████████████████████████████████| 757kB 8.0MB/s \n",
    "    Collecting sacremoses\n",
    "      Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
    "         |████████████████████████████████| 890kB 48.3MB/s \n",
    "    Collecting tokenizers==0.8.0-rc4\n",
    "      Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
    "         |████████████████████████████████| 3.0MB 55.4MB/s \n",
    "    Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.8)\n",
    "    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
    "    Collecting sentencepiece\n",
    "      Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
    "         |████████████████████████████████| 1.2MB 61.3MB/s \n",
    "    Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8)\n",
    "    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.19.5)\n",
    "    Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n",
    "    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n",
    "    Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n",
    "    Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
    "    Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
    "    Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.0.0)\n",
    "    Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
    "    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
    "    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n",
    "    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
    "    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
    "    Building wheels for collected packages: sacremoses\n",
    "      Building wheel for sacremoses (setup.py) ... done\n",
    "      Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=4e0add7e2ed4067ea2a1130855b2172e74e01b1b27789ad494c280182a0bfd3b\n",
    "      Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
    "    Successfully built sacremoses\n",
    "    Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
    "    Successfully installed sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.8.0rc4 transformers-3.0.0\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    import torch\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    from transformers import BertTokenizer\n",
    "    from torch.utils.data import TensorDataset\n",
    "\n",
    "    from transformers import BertForSequenceClassification\n",
    "\n",
    "# BERT models For refence only<a href=\"#BERT-models-For-refence-only\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "    map_name_to_handle = {\n",
    "        'bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "        'bert_en_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "        'bert_multi_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "        'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "        'albert_en_base':\n",
    "            'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "        'electra_small':\n",
    "            'https://tfhub.dev/google/electra_small/2',\n",
    "        'electra_base':\n",
    "            'https://tfhub.dev/google/electra_base/2',\n",
    "        'experts_pubmed':\n",
    "            'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "        'experts_wiki_books':\n",
    "            'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "        'talking-heads_base':\n",
    "            'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "    }\n",
    "\n",
    "    map_model_to_preprocess = {\n",
    "        'bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'bert_en_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'bert_multi_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/2',\n",
    "        'albert_en_base':\n",
    "            'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
    "        'electra_small':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'electra_base':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'experts_pubmed':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'experts_wiki_books':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'talking-heads_base':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    }\n",
    "\n",
    "    tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "    tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "    print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "    print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n",
    "\n",
    "    BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
    "    Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\n",
    "\n",
    "# Load dataset<a href=\"#Load-dataset\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    # Read the data as a data frame\n",
    "\n",
    "    #dataframe = pd.read_csv('dataframe_translated_20210102_2.csv')\n",
    "    dataframe2 = pd.read_csv('dataframe_translated_20210102_2.csv')\n",
    "    dataframe2.head()\n",
    "\n",
    "Out\\[10\\]:\n",
    "\n",
    "|     | index | Short description                                 | Description                                       | Caller            | Assignment group | PP Short description                       | PP Description                                    | Grp No | Language    | Language Code | temp desc                                           | Language 2  | Language Code 2 | English Short description                  | English Description                              | Caller Encoded |\n",
    "|-----|-------|---------------------------------------------------|---------------------------------------------------|-------------------|------------------|--------------------------------------------|---------------------------------------------------|--------|-------------|---------------|-----------------------------------------------------|-------------|-----------------|--------------------------------------------|--------------------------------------------------|----------------|\n",
    "| 0   | 0     | skype error                                       | skype error                                       | owlgqjme qhcozdfx | GRP_0            | skype error                                | skype error                                       | 0      | Latin       | la            | skype error \\|\\| skype error                        | Latin       | la              | skype error                                | skype error                                      | Caller231      |\n",
    "| 1   | 1     | erp_print_tool install.                           | erp_print_tool install.                           | aorthyme rnsuipbk | GRP_0            | erp_print_tool install .                   | erp_print_tool install .                          | 0      | Kinyarwanda | rw            | erp_print_tool install . \\|\\| erp_print_tool ins... | Kinyarwanda | rw              | erp_print_tool install .                   | erp_print_tool install .                         | Caller35       |\n",
    "| 2   | 2     | probleme mit bluescreen .                         | hallo ,\\n\\nes ist erneut passiert. der pc hat ... | vrfpyjwi nzhvgqiw | GRP_24           | probleme mit bluescreen .                  | hallo , es ist erneut passiert . der pc hat si... | 24     | German      | de            | problems with bluescreen. \\|\\| hello, it happene... | English     | en              | problems with bluescreen.                  | hello, it happened again. the pc hung up agai... | Caller284      |\n",
    "| 3   | 3     | reset the password for fygrwuna gomcekzi on e-... | bitte passwort fÃ¼r fygrwuna gomcekzi e-mail z... | fygrwuna gomcekzi | GRP_0            | reset the password for Caller690 on e mail | bitte passwort fa r Caller690 e mail zura ckse... | 0      | German      | de            | reset the password for Caller690 on e mail \\|\\| ... | English     | en              | reset the password for Caller690 on e mail | please reset password for Caller690 e mail, p... | Caller690      |\n",
    "| 4   | 4     | probleme mit laufwerk z: \\laeusvjo fvaihgpx       | probleme mit laufwerk z: \\laeusvjo fvaihgpx       | laeusvjo fvaihgpx | GRP_24           | probleme mit laufwerk z Caller663          | probleme mit laufwerk z Caller663                 | 24     | German      | de            | problems with drive z Caller663 \\|\\| problems wi... | English     | en              | problems with drive z Caller663            | problems with drive z Caller663                  | Caller663      |\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    y_counts = pd.DataFrame(dataframe2['Grp No'].value_counts())\n",
    "    p = y_counts.index.values\n",
    "    y_counts.insert( 0, column=\"new\",value = p)\n",
    "    y_counts.columns = ['Grp No', 'counts']\n",
    "    y_counts['Grp No'].astype(int)\n",
    "    y_counts['counts'].astype(int)\n",
    "    y_counts\n",
    "\n",
    "Out\\[11\\]:\n",
    "\n",
    "|     | Grp No | counts |\n",
    "|-----|--------|--------|\n",
    "| 0   | 0      | 3976   |\n",
    "| 8   | 8      | 661    |\n",
    "| 24  | 24     | 289    |\n",
    "| 12  | 12     | 257    |\n",
    "| 9   | 9      | 252    |\n",
    "| ... | ...    | ...    |\n",
    "| 61  | 61     | 1      |\n",
    "| 67  | 67     | 1      |\n",
    "| 35  | 35     | 1      |\n",
    "| 70  | 70     | 1      |\n",
    "| 73  | 73     | 1      |\n",
    "\n",
    "74 rows × 2 columns\n",
    "\n",
    "In \\[20\\]:\n",
    "\n",
    "    y_counts.head(50)\n",
    "\n",
    "Out\\[20\\]:\n",
    "\n",
    "|     | Grp No | counts |\n",
    "|-----|--------|--------|\n",
    "| 0   | 0      | 3976   |\n",
    "| 8   | 8      | 661    |\n",
    "| 24  | 24     | 289    |\n",
    "| 12  | 12     | 257    |\n",
    "| 9   | 9      | 252    |\n",
    "| 2   | 2      | 241    |\n",
    "| 19  | 19     | 215    |\n",
    "| 3   | 3      | 200    |\n",
    "| 6   | 6      | 184    |\n",
    "| 13  | 13     | 145    |\n",
    "| 10  | 10     | 140    |\n",
    "| 5   | 5      | 129    |\n",
    "| 14  | 14     | 118    |\n",
    "| 25  | 25     | 116    |\n",
    "| 33  | 33     | 107    |\n",
    "| 4   | 4      | 100    |\n",
    "| 29  | 29     | 97     |\n",
    "| 18  | 18     | 88     |\n",
    "| 16  | 16     | 85     |\n",
    "| 17  | 17     | 81     |\n",
    "| 31  | 31     | 69     |\n",
    "| 7   | 7      | 68     |\n",
    "| 34  | 34     | 62     |\n",
    "| 26  | 26     | 56     |\n",
    "| 40  | 40     | 45     |\n",
    "| 28  | 28     | 44     |\n",
    "| 41  | 41     | 40     |\n",
    "| 30  | 30     | 39     |\n",
    "| 15  | 15     | 39     |\n",
    "| 42  | 42     | 37     |\n",
    "| 20  | 20     | 36     |\n",
    "| 45  | 45     | 35     |\n",
    "| 22  | 22     | 31     |\n",
    "| 1   | 1      | 31     |\n",
    "| 11  | 11     | 30     |\n",
    "| 21  | 21     | 29     |\n",
    "| 47  | 47     | 27     |\n",
    "| 48  | 48     | 25     |\n",
    "| 23  | 23     | 25     |\n",
    "| 62  | 62     | 25     |\n",
    "| 60  | 60     | 20     |\n",
    "| 39  | 39     | 19     |\n",
    "| 27  | 27     | 18     |\n",
    "| 37  | 37     | 16     |\n",
    "| 36  | 36     | 15     |\n",
    "| 44  | 44     | 15     |\n",
    "| 50  | 50     | 14     |\n",
    "| 65  | 65     | 11     |\n",
    "| 53  | 53     | 11     |\n",
    "| 52  | 52     | 9      |\n",
    "\n",
    "In \\[32\\]:\n",
    "\n",
    "    #incidentsData_Group_0 = dataframe2[dataframe2['Assignment group'] == 'GRP_0']\n",
    "    #incidentsData_Others = dataframe2[dataframe2['Assignment group'] != 'GRP_0']\n",
    "    #max_incident_cnt = dataframe2['Assignment group'].value_counts().max()\n",
    "\n",
    "    no_upsampling_grp = ['GRP_0', 'GRP_8', 'GRP_24', 'GRP_12', 'GRP_9']\n",
    "    incidentsData_no_upsample = dataframe2[dataframe2['Assignment group'].isin(no_upsampling_grp)]\n",
    "    incidentsData_Others = dataframe2[~dataframe2['Assignment group'].isin(no_upsampling_grp)]\n",
    "\n",
    "In \\[34\\]:\n",
    "\n",
    "    #incidentsData_Others.shape\n",
    "    incidentsData_no_upsample.shape\n",
    "\n",
    "Out\\[34\\]:\n",
    "\n",
    "    (5435, 16)\n",
    "\n",
    "In \\[36\\]:\n",
    "\n",
    "    # Treat the imbalance in the 'other' dataset by resampling\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "    incidentsData_upsampled = incidentsData_Others[0:0]\n",
    "\n",
    "    # Upsample minority class\n",
    "    for grp in incidentsData_Others['Assignment group'].unique():\n",
    "        incidentsData_Group = incidentsData_Others[incidentsData_Others['Assignment group'] == grp]\n",
    "        resampled = resample(incidentsData_Group, \n",
    "                             replace=True, # sample with replacement\n",
    "                             #n_samples=int(max_incident_cnt/2), \n",
    "                             n_samples=int(250), \n",
    "                             random_state=123) # reproducible results\n",
    "        \n",
    "        incidentsData_upsampled = incidentsData_upsampled.append(resampled)\n",
    "\n",
    "    incidentsData_Others_upsample = pd.concat([incidentsData_no_upsample,incidentsData_upsampled])\n",
    "    incidentsData_Others_upsample.reset_index(inplace=True)\n",
    "\n",
    "    descending_order = incidentsData_upsampled['Assignment group'].value_counts().sort_values(ascending=False).index\n",
    "\n",
    "In \\[37\\]:\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    from wordcloud import WordCloud, STOPWORDS \n",
    "    import seaborn as sns\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    flatui = ['#2E82A8','#00A0B8','#00BDB4','#53D69F','#A5EB84','#F9F871']\n",
    "\n",
    "In \\[38\\]:\n",
    "\n",
    "    plt.subplots(figsize=(22,5))\n",
    "    #add code to rotate the labels\n",
    "    ax=sns.countplot(x='Assignment group', data=incidentsData_upsampled, palette = sns.color_palette(flatui),order=descending_order)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "![](data:image/png;base64,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%0A)\n",
    "\n",
    "In \\[40\\]:\n",
    "\n",
    "    incidentsData_Others_upsample.shape\n",
    "\n",
    "Out\\[40\\]:\n",
    "\n",
    "    (22685, 17)\n",
    "\n",
    "In \\[41\\]:\n",
    "\n",
    "    possible_labels = incidentsData_Others_upsample['Assignment group'].unique()\n",
    "\n",
    "    label_dict = {}\n",
    "    for index, possible_label in enumerate(possible_labels):\n",
    "        label_dict[possible_label] = index\n",
    "    label_dict\n",
    "\n",
    "Out\\[41\\]:\n",
    "\n",
    "    {'GRP_0': 0,\n",
    "     'GRP_1': 35,\n",
    "     'GRP_10': 26,\n",
    "     'GRP_11': 22,\n",
    "     'GRP_12': 2,\n",
    "     'GRP_13': 15,\n",
    "     'GRP_14': 17,\n",
    "     'GRP_15': 29,\n",
    "     'GRP_16': 24,\n",
    "     'GRP_17': 40,\n",
    "     'GRP_18': 27,\n",
    "     'GRP_19': 18,\n",
    "     'GRP_2': 8,\n",
    "     'GRP_20': 30,\n",
    "     'GRP_21': 41,\n",
    "     'GRP_22': 42,\n",
    "     'GRP_23': 19,\n",
    "     'GRP_24': 1,\n",
    "     'GRP_25': 6,\n",
    "     'GRP_26': 21,\n",
    "     'GRP_27': 43,\n",
    "     'GRP_28': 5,\n",
    "     'GRP_29': 25,\n",
    "     'GRP_3': 36,\n",
    "     'GRP_30': 14,\n",
    "     'GRP_31': 10,\n",
    "     'GRP_32': 32,\n",
    "     'GRP_33': 7,\n",
    "     'GRP_34': 12,\n",
    "     'GRP_35': 44,\n",
    "     'GRP_36': 9,\n",
    "     'GRP_37': 45,\n",
    "     'GRP_38': 46,\n",
    "     'GRP_39': 47,\n",
    "     'GRP_4': 37,\n",
    "     'GRP_40': 48,\n",
    "     'GRP_41': 49,\n",
    "     'GRP_42': 11,\n",
    "     'GRP_43': 50,\n",
    "     'GRP_44': 51,\n",
    "     'GRP_45': 34,\n",
    "     'GRP_46': 33,\n",
    "     'GRP_47': 52,\n",
    "     'GRP_48': 16,\n",
    "     'GRP_49': 13,\n",
    "     'GRP_5': 28,\n",
    "     'GRP_50': 53,\n",
    "     'GRP_51': 54,\n",
    "     'GRP_52': 31,\n",
    "     'GRP_53': 55,\n",
    "     'GRP_54': 56,\n",
    "     'GRP_55': 57,\n",
    "     'GRP_56': 58,\n",
    "     'GRP_57': 59,\n",
    "     'GRP_58': 60,\n",
    "     'GRP_59': 23,\n",
    "     'GRP_6': 38,\n",
    "     'GRP_60': 61,\n",
    "     'GRP_61': 62,\n",
    "     'GRP_62': 20,\n",
    "     'GRP_63': 63,\n",
    "     'GRP_64': 64,\n",
    "     'GRP_65': 65,\n",
    "     'GRP_66': 66,\n",
    "     'GRP_67': 67,\n",
    "     'GRP_68': 68,\n",
    "     'GRP_69': 69,\n",
    "     'GRP_7': 39,\n",
    "     'GRP_70': 70,\n",
    "     'GRP_71': 71,\n",
    "     'GRP_72': 72,\n",
    "     'GRP_73': 73,\n",
    "     'GRP_8': 3,\n",
    "     'GRP_9': 4}\n",
    "\n",
    "In \\[42\\]:\n",
    "\n",
    "    df = incidentsData_Others_upsample.copy()\n",
    "\n",
    "In \\[43\\]:\n",
    "\n",
    "    df['label'] = df['Assignment group'].replace(label_dict)\n",
    "\n",
    "In \\[44\\]:\n",
    "\n",
    "    df.head()\n",
    "\n",
    "Out\\[44\\]:\n",
    "\n",
    "|     | level_0 | index | Short description                                 | Description                                       | Caller            | Assignment group | PP Short description                       | PP Description                                    | Grp No | Language    | Language Code | temp desc                                           | Language 2  | Language Code 2 | English Short description                  | English Description                              | Caller Encoded | label |\n",
    "|-----|---------|-------|---------------------------------------------------|---------------------------------------------------|-------------------|------------------|--------------------------------------------|---------------------------------------------------|--------|-------------|---------------|-----------------------------------------------------|-------------|-----------------|--------------------------------------------|--------------------------------------------------|----------------|-------|\n",
    "| 0   | 0       | 0     | skype error                                       | skype error                                       | owlgqjme qhcozdfx | GRP_0            | skype error                                | skype error                                       | 0      | Latin       | la            | skype error \\|\\| skype error                        | Latin       | la              | skype error                                | skype error                                      | Caller231      | 0     |\n",
    "| 1   | 1       | 1     | erp_print_tool install.                           | erp_print_tool install.                           | aorthyme rnsuipbk | GRP_0            | erp_print_tool install .                   | erp_print_tool install .                          | 0      | Kinyarwanda | rw            | erp_print_tool install . \\|\\| erp_print_tool ins... | Kinyarwanda | rw              | erp_print_tool install .                   | erp_print_tool install .                         | Caller35       | 0     |\n",
    "| 2   | 2       | 2     | probleme mit bluescreen .                         | hallo ,\\n\\nes ist erneut passiert. der pc hat ... | vrfpyjwi nzhvgqiw | GRP_24           | probleme mit bluescreen .                  | hallo , es ist erneut passiert . der pc hat si... | 24     | German      | de            | problems with bluescreen. \\|\\| hello, it happene... | English     | en              | problems with bluescreen.                  | hello, it happened again. the pc hung up agai... | Caller284      | 1     |\n",
    "| 3   | 3       | 3     | reset the password for fygrwuna gomcekzi on e-... | bitte passwort fÃ¼r fygrwuna gomcekzi e-mail z... | fygrwuna gomcekzi | GRP_0            | reset the password for Caller690 on e mail | bitte passwort fa r Caller690 e mail zura ckse... | 0      | German      | de            | reset the password for Caller690 on e mail \\|\\| ... | English     | en              | reset the password for Caller690 on e mail | please reset password for Caller690 e mail, p... | Caller690      | 0     |\n",
    "| 4   | 4       | 4     | probleme mit laufwerk z: \\laeusvjo fvaihgpx       | probleme mit laufwerk z: \\laeusvjo fvaihgpx       | laeusvjo fvaihgpx | GRP_24           | probleme mit laufwerk z Caller663          | probleme mit laufwerk z Caller663                 | 24     | German      | de            | problems with drive z Caller663 \\|\\| problems wi... | English     | en              | problems with drive z Caller663            | problems with drive z Caller663                  | Caller663      | 1     |\n",
    "\n",
    "In \\[45\\]:\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                      df.label.values, \n",
    "                                                      test_size=0.15, \n",
    "                                                      random_state=42, \n",
    "                                                      stratify=df.label.values)\n",
    "\n",
    "In \\[46\\]:\n",
    "\n",
    "    df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "    df.loc[X_train, 'data_type'] = 'train'\n",
    "    df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "In \\[47\\]:\n",
    "\n",
    "    df.groupby(['Assignment group', 'label', 'data_type']).count()\n",
    "\n",
    "Out\\[47\\]:\n",
    "\n",
    "|                  |       |           | level_0 | index | Short description | Description | Caller | PP Short description | PP Description | Grp No | Language | Language Code | temp desc | Language 2 | Language Code 2 | English Short description | English Description | Caller Encoded |\n",
    "|------------------|-------|-----------|---------|-------|-------------------|-------------|--------|----------------------|----------------|--------|----------|---------------|-----------|------------|-----------------|---------------------------|---------------------|----------------|\n",
    "| Assignment group | label | data_type |         |       |                   |             |        |                      |                |        |          |               |           |            |                 |                           |                     |                |\n",
    "| GRP_0            | 0     | train     | 3380    | 3380  | 3380              | 3380        | 3380   | 3380                 | 3378           | 3380   | 3380     | 3380          | 3380      | 3380       | 3380            | 3380                      | 3380                | 3380           |\n",
    "|                  |       | val       | 596     | 596   | 596               | 596         | 596    | 596                  | 596            | 596    | 596      | 596           | 596       | 596        | 596             | 596                       | 596                 | 596            |\n",
    "| GRP_1            | 35    | train     | 213     | 213   | 213               | 213         | 213    | 213                  | 213            | 213    | 213      | 213           | 213       | 213        | 213             | 213                       | 213                 | 213            |\n",
    "|                  |       | val       | 37      | 37    | 37                | 37          | 37     | 37                   | 37             | 37     | 37       | 37            | 37        | 37         | 37              | 37                        | 37                  | 37             |\n",
    "| GRP_10           | 26    | train     | 212     | 212   | 212               | 212         | 212    | 212                  | 212            | 212    | 212      | 212           | 212       | 212        | 212             | 212                       | 212                 | 212            |\n",
    "| ...              | ...   | ...       | ...     | ...   | ...               | ...         | ...    | ...                  | ...            | ...    | ...      | ...           | ...       | ...        | ...             | ...                       | ...                 | ...            |\n",
    "| GRP_73           | 73    | val       | 37      | 37    | 37                | 37          | 37     | 37                   | 37             | 37     | 37       | 37            | 37        | 37         | 37              | 37                        | 37                  | 37             |\n",
    "| GRP_8            | 3     | train     | 562     | 562   | 562               | 562         | 562    | 562                  | 562            | 562    | 562      | 562           | 562       | 562        | 562             | 562                       | 562                 | 562            |\n",
    "|                  |       | val       | 99      | 99    | 99                | 99          | 99     | 99                   | 99             | 99     | 99       | 99            | 99        | 99         | 99              | 99                        | 99                  | 99             |\n",
    "| GRP_9            | 4     | train     | 214     | 214   | 214               | 214         | 214    | 214                  | 214            | 214    | 214      | 214           | 214       | 214        | 214             | 214                       | 214                 | 214            |\n",
    "|                  |       | val       | 38      | 38    | 38                | 38          | 38     | 38                   | 38             | 38     | 38       | 38            | 38        | 38         | 38              | 38                        | 38                  | 38             |\n",
    "\n",
    "148 rows × 16 columns\n",
    "\n",
    "In \\[48\\]:\n",
    "\n",
    "    df['CD'] = df['Short description'] + ' . ' + df['Description']\n",
    "\n",
    "In \\[49\\]:\n",
    "\n",
    "    fig = plt.figure(figsize=(6,9))\n",
    "    text_len=df['CD'].str.split().map(lambda x: len(str(x).split(\" \")))\n",
    "    sns.displot(text_len.dropna(),color='#00A0B8',binwidth=50)\n",
    "    fig.suptitle('Words in description')\n",
    "    plt.show()\n",
    "\n",
    "    <Figure size 432x648 with 0 Axes>\n",
    "\n",
    "![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dcVDUd37/8efCHlRDAoruMvJDUxru6ihC2zMNlcBknYUoEojCXL1JZqSmTpVo1ZubE9NDg8aY5CZnjDNWaidnb5xrE0+h595djCSKXJKSpDpIunY0CT3IHLs9FAzSBRe/vz+c7MgJBAjLZ9HXY8aZ3fd+9vt9f77rvvjy3e93sVmWZSEiIhMuynQDIiJ3KwWwiIghCmAREUMUwCIihiiARUQMuesC+OLFi6Ma39LSEp5Gxkkk96fexi6S+4vk3iDy+7vVXRfAwWBwVOP/7//+L0ydjI9I7k+9jV0k9xfJvUHk93eruy6ARUQihQJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAxRAIuIGKIAFhExRAEsImKIAlhExBAFsIiIIQpgERFD7OFacEVFBadOnSIxMZHjx48DsHHjRj777DMAvvjiC+69915qa2tpa2tj6dKl/PEf/zEAGRkZVFVVAdDc3ExFRQWBQIDc3FyeeeYZbDYbnZ2dbNq0ic8//5zk5GT27NlDfHx8uKYjIjLuwhbAy5cv54knnuAHP/hBqLZnz57Q7d27dxMXFxe6P3v2bGpra29bzvbt29mxYwcZGRn87d/+LfX19eTm5lJdXU1WVhZr1qyhurqa6upqvv/974dlLt3Xg/QP87dLo2024r4Rtk0pIneosKXGwoULaWtrG/Qxy7L41a9+xaFDh4Zdht/vp7u7m8zMTACKi4upq6sjNzeXuro6fvrTn4bqTz75ZNgCuN+yKH7n7JCP1zzyZ2FZr4jc2YwcA/7www9JTEzk/vvvD9Xa2tooLi7miSee4MMPPwTA5/ORlJQUGpOUlITP5wOgo6MDh8MBwMyZM+no6Ji4CYiIjAMjvzcfP36cZcuWhe47HA7eeecdpk2bRnNzM+Xl5Xg8nhEvz2azYbPZRjS2t7cXr9c74mUHAgGCwSA913qGHBMMBvF+cmnEyxxPgUBgVPOZSOpt7CK5v0juDSKzv7lz5w5an/AADgaDvPXWWxw9ejRUi4mJISYmBoD58+cze/ZsPvvsM5xOJ+3t7aFx7e3tOJ1OABITE/H7/TgcDvx+P9OnTx/R+mNjY4fcGIPxer3Y7Xam3jN1yDF2u31UyxxPXq/X2Lq/inobu0juL5J7g8jv71YTfgji3XffJTU1dcChhcuXL9Pf3w9Aa2srLS0tpKSk4HA4iIuL49y5c1iWRU1NDYsXLwbA5XJRU1MDMKAuIjJZhG0PePPmzTQ2NnLlyhVycnJYv349paWl/PKXv6SgoGDA2A8++IC9e/dit9uJiori2WefJSEhAYBt27aFTkPLyckhJycHgDVr1rBx40aOHDnCrFmzBpxhISIyGYQtgF9++eVB67t3776tlp+fT35+/qDj09PTQ+cR32ratGlfeRaFiEgk05VwIiKGKIBFRAxRAIuIGKIAFhExRAEsImKIAlhExBAFsIiIIQpgERFDFMAiIoYogEVEDFEAi4gYogAWETFEASwiYogCWETEEAWwiIghCmAREUMUwCIihiiARUQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAxRAIuIGKIAFhExRAEsImKIAlhExBAFsIiIIQpgERFDFMAiIoaELYArKirIyspi2bJlodqrr77Kww8/TFFREUVFRZw+fTr02IEDB3C73eTn53PmzJlQvb6+nvz8fNxuN9XV1aF6a2srpaWluN1uNm7cSF9fX7imIiISFmEL4OXLl3Pw4MHb6qtWraK2tpba2lpyc3MBuHTpEh6PB4/Hw8GDB3n22Wfp7++nv7+fqqoqDh48iMfj4fjx41y6dAmAH/3oR6xatYq33nqL++67jyNHjoRrKiIiYRG2AF64cCHx8fEjGltXV0dBQQExMTGkpKQwZ84cmpqaaGpqYs6cOaSkpBATE0NBQQF1dXVYlsX7779Pfn4+AI8//jh1dXXhmoqISFjYJ3qFhw8fpqamhvnz57Nlyxbi4+Px+XxkZGSExjidTnw+HwBJSUkD6k1NTVy5coX77rsPu90eGvPl+K/S29uL1+sdcb+BQIBgMEjPtZ4hxwSDQbyfXBrxMsdTIBAY1Xwmknobu0juL5J7g8jsb+7cuYPWJzSAV65cybp167DZbLzyyivs3r2b559/fiJbIDY2dsiNMRiv14vdbmfqPVOHHGO320e1zPHk9XqNrfurqLexi+T+Irk3iPz+bjWhZ0HMmDGD6OhooqKiKC0t5fz588DNPdv29vbQOJ/Ph9PpHLI+bdo0rl69SjAYBKC9vR2n0zmRUxER+domNID9fn/o9smTJ0lLSwPA5XLh8Xjo6+ujtbWVlpYWFixYQHp6Oi0tLbS2ttLX14fH48HlcmGz2fjLv/xL3nzzTQCOHTuGy+WayKmIiHxtYTsEsXnzZhobG7ly5Qo5OTmsX7+exsZGLly4AEBycjJVVVUApKWlsWTJEpYuXUp0dDSVlZVER0cDUFlZyVNPPUV/fz8rVqwIhfb3v/99Nm3axJ49e5g7dy6lpaXhmoqISFiELYBffvnl22rDheTatWtZu3btbfXc3NzQ6Wq3SklJ0alnIjKp6Uo4ERFDFMAiIoYogEVEDFEAi4gYogAWETFEASwiYogCWETEEAWwiIghCmAREUMUwCIihiiARUQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAxRAIuIGKIAFhExRAEsImKIAlhExBAFsIiIIQpgERFDFMAiIoYogEVEDFEAi4gYogAWETFEASwiYogCWETEEAWwiIghYQvgiooKsrKyWLZsWaj2wgsv8Oijj1JYWEh5eTlXr14FoK2tjQULFlBUVERRURGVlZWh5zQ3N1NYWIjb7Wbnzp1YlgVAZ2cnZWVl5OXlUVZWRldXV7imIiISFmEL4OXLl3Pw4MEBtUWLFnH8+HF+8YtfcP/993PgwIHQY7Nnz6a2tpba2lqqqqpC9e3bt7Njxw5OnDhBS0sL9fX1AFRXV5OVlcWJEyfIysqiuro6XFMREQmLsAXwwoULiY+PH1DLzs7GbrcDkJmZSXt7+7DL8Pv9dHd3k5mZic1mo7i4mLq6OgDq6uooLi4GoLi4mJMnT4ZhFiIi4WM3teKf//znLFmyJHS/ra2N4uJi4uLi2LhxI9/+9rfx+XwkJSWFxiQlJeHz+QDo6OjA4XAAMHPmTDo6Oka03t7eXrxe74j7DAQCBINBeq71DDkmGAzi/eTSiJc5ngKBwKjmM5HU29hFcn+R3BtEZn9z584dtG4kgPfv3090dDSPPfYYAA6Hg3feeYdp06bR3NxMeXk5Ho9nxMuz2WzYbLYRjY2NjR1yYwzG6/Vit9uZes/UIcfY7fZRLXM8eb1eY+v+Kupt7CK5v0juDSK/v1tNeAAfPXqUU6dO8ZOf/CQUmjExMcTExAAwf/58Zs+ezWeffYbT6RxwmKK9vR2n0wlAYmIifr8fh8OB3+9n+vTpEz0VEZGvZUJPQ6uvr+fgwYPs37+fKVOmhOqXL1+mv78fgNbWVlpaWkhJScHhcBAXF8e5c+ewLIuamhoWL14MgMvloqamBmBAXURksgjbHvDmzZtpbGzkypUr5OTksH79eqqrq+nr66OsrAyAjIwMqqqq+OCDD9i7dy92u52oqCieffZZEhISANi2bRsVFRUEAgFycnLIyckBYM2aNWzcuJEjR44wa9Ys9uzZE66piIiERdgC+OWXX76tVlpaOujY/Px88vPzB30sPT2d48eP31afNm0ahw4d+npNiogYpCvhREQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAxRAIuIGKIAFhExRAEsImKIAlhExBAFsIiIIQpgERFDFMAiIoYogEVEDFEAi4gYogAWETFEASwiYogCWETEEAWwiIghCmAREUMUwCIihiiARUQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAwJawBXVFSQlZXFsmXLQrXOzk7KysrIy8ujrKyMrq4uACzLYufOnbjdbgoLC/n4449Dzzl27Bh5eXnk5eVx7NixUL25uZnCwkLcbjc7d+7EsqxwTkdEZFyFNYCXL1/OwYMHB9Sqq6vJysrixIkTZGVlUV1dDUB9fT0tLS2cOHGCHTt2sH37duBmYO/bt4/XX3+dN954g3379oVCe/v27ezYsYMTJ07Q0tJCfX19OKcjIjKuwhrACxcuJD4+fkCtrq6O4uJiAIqLizl58uSAus1mIzMzk6tXr+L3+2loaGDRokUkJCQQHx/PokWLOHPmDH6/n+7ubjIzM7HZbBQXF1NXVxfO6YiIjCv7SAZ99NFH/MVf/MVX1kaio6MDh8MBwMyZM+no6ADA5/ORlJQUGpeUlITP57ut7nQ6B61/Of6r9Pb24vV6R9xvIBAgGAzSc61nyDHBYBDvJ5dGvMzxFAgERjWfiaTexi6S+4vk3iAy+5s7d+6g9REF8M6dOwccex2qNlo2mw2bzfa1ljFasbGxQ26MwXi9Xux2O1PvmTrkGLvdPqpljiev12ts3V9FvY1dJPcXyb1B5Pd3q2ED+OzZs5w9e5bLly/z2muvherd3d309/ePaYWJiYn4/X4cDgd+v5/p06cDN/ds29vbQ+Pa29txOp04nU4aGxtDdZ/Px4MPPjjkeBGRyWLYY8DXr1+np6eH/v5+rl27FvoXFxfH3r17x7RCl8tFTU0NADU1NSxevHhA3bIszp07x7333ovD4SA7O5uGhga6urro6uqioaGB7OxsHA4HcXFxnDt3DsuyBixLRGQyGHYP+MEHH+TBBx/k8ccfJzk5edQL37x5M42NjVy5coWcnBzWr1/PmjVr2LhxI0eOHGHWrFns2bMHgNzcXE6fPo3b7WbKlCns2rULgISEBNatW0dJSQkA5eXlJCQkALBt2zYqKioIBALk5OSQk5Mz6h5FREwZ0THgvr4+fvjDH/L5558TDAZD9X/5l38Z9nkvv/zyoPVDhw7dVrPZbGzbtm3Q8SUlJaEAvlV6ejrHjx8ftgcRkUg1ogD++7//e/76r/+a0tJSoqJ08ZyIyHgYUQDb7Xa++93vhrsXEZG7yoh2Zx955BEOHz6M3++ns7Mz9E9ERMZuRHvAX57v+8///M+hms1m05VnIiJfw4gC+O233w53HyIid50RBfCX5+3+oS+/00FEREZvRAF8/vz50O3e3l7ee+895s2bpwAWEfkaRhTAP/zhDwfcv3r1Kps2bQpLQyIid4sxndQ7ZcoU2traxrsXEZG7yoj2gP/u7/4udPvGjRt88sknLFmyJGxNiYjcDUYUwH/zN38Tuh0dHU1ycvKA7+IVEZHRG9EhiAcffJDU1FSuXbvG1atX+cY3vhHuvkRE7ngjCuBf/vKXlJaW8utf/5pf/epXodsiIjJ2IzoE8Y//+I8cOXKExMREAC5fvsyqVat49NFHw9qciMidbER7wJZlhcIXbn5Hr/4EvIjI1zOiPeDs7GxWr15NQUEBcPOQhL78XETk6xk2gP/nf/6H3//+9/zgBz/gxIkTfPTRRwBkZmby2GOPTUiDIiJ3qmEPQezatYu4uDgA8vLyqKiooKKiArfbHfqTQSIiMjbDBvDvf/97vvWtb91W/9a3vsXnn38etqZERO4GwwbwF198MeRjgUBg3JsREbmbDBvA8+fP5/XXX7+t/sYbbzBv3rywNSUicjcY9kO4rVu38vTTT/OLX/wiFLjNzc1cv36dffv2TUiDIiJ3qmEDeMaMGfzrv/4r77//PhcvXgQgNzeXrKysCWlORORONqLzgB966CEeeuihcPciInJXGdP3AYuIyNenABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAwZ0YUY4+nTTz9l06ZNofutra1s2LCBL774gtdff53p06cDsHnzZnJzcwE4cOAAR44cISoqin/4h3/g4YcfBqC+vp7nnnuOGzduUFpaypo1ayZ6OiIiYzbhAZyamkptbS0A/f395OTk4Ha7OXr0KKtWrWL16tUDxl+6dAmPx4PH48Hn81FWVsabb74JQFVVFa+99hpOp5OSkhJcLhcPPPDARE9JRGRMJjyAb/Xee++RkpJCcnLykGPq6uooKCggJiaGlJQU5syZQ1NTEwBz5swhJSUFgIKCAurq6hTAIjJpGA1gj8fDsmXLQvcPHz5MTU0N8+fPZ8uWLcTHx+Pz+cjIyAiNcTqd+Hw+AJKSkgbUvwzm4fT29uL1ekfcYyAQIBgM0nOtZ8gxwWAQ7yeXRrzM8RQIBEY1n4mk3sYukvuL5N4gMvubO3fuoHVjAdzX18fbb7/N9773PQBWrlzJunXrsNlsvPLKK+zevZvnn39+3NcbGxs75MYYjNfrxW63M/WeqUOOsdvto1rmePJ6vcbW/VXU29hFcn+R3BtEfn+3MnYWRH19PfPmzWPGjBnAza++jI6OJioqitLSUs6fPw/c3LNtb28PPc/n8+F0Ooesi4hMFsYC2OPxhP7MPYDf7w/dPnnyJGlpaQC4XC48Hg99fX20trbS0tLCggULSE9Pp6WlhdbWVvr6+vB4PLhcrgmfh4jIWBk5BNHT08O7775LVVVVqPbSSy9x4cIFAJKTk0OPpaWlsWTJEpYuXUp0dDSVlZVER0cDUFlZyVNPPUV/fz8rVqwIhbaIyGRgJICnTp3Kf/zHfwyovfTSS0OOX7t2LWvXrr2tnpubGzpXWERkstGVcCIihiiARUQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAxRAIuIGKIAFhExRAEsImKIAlhExBAFsIiIIQpgERFDFMAiIoYogEVEDFEAi4gYogAWETFEASwiYogCWETEEAWwiIghCmAREUMUwCIihiiARUQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKG2E2t2OVycc899xAVFUV0dDRHjx6ls7OTTZs28fnnn5OcnMyePXuIj4/Hsiyee+45Tp8+zR/90R+xe/du5s2bB8CxY8fYv38/AGvXruXxxx83NSURkVExugd86NAhamtrOXr0KADV1dVkZWVx4sQJsrKyqK6uBqC+vp6WlhZOnDjBjh072L59OwCdnZ3s27eP119/nTfeeIN9+/bR1dVlajoiIqMSUYcg6urqKC4uBqC4uJiTJ08OqNtsNjIzM7l69Sp+v5+GhgYWLVpEQkIC8fHxLFq0iDNnzpicgojIiBk7BAGwevVqbDYb3/nOd/jOd75DR0cHDocDgJkzZ9LR0QGAz+cjKSkp9LykpCR8Pt9tdafTic/nG3advb29eL3eEfcYCAQIBoP0XOsZckwwGMT7yaURL3M8BQKBUc1nIqm3sYvk/iK5N4jM/ubOnTto3VgA/+xnP8PpdNLR0UFZWRmpqakDHrfZbNhstnFfb2xs7JAbYzBerxe73c7Ue6YOOcZut49qmePJ6/UaW/dXUW9jF8n9RXJvEPn93crYIQin0wlAYmIibrebpqYmEhMT8fv9APj9fqZPnx4a297eHnpue3s7TqfztrrP5wstV0Qk0hkJ4J6eHrq7u0O3f/Ob35CWlobL5aKmpgaAmpoaFi9eDBCqW5bFuXPnuPfee3E4HGRnZ9PQ0EBXVxddXV00NDSQnZ1tYkoiIqNm5BBER0cH5eXlAPT397Ns2TJycnJIT09n48aNHDlyhFmzZrFnzx4AcnNzOX36NG63mylTprBr1y4AEhISWLduHSUlJQCUl5eTkJBgYkoiIqNmJIBTUlL493//99vq06ZN49ChQ7fVbTYb27ZtG3RZJSUloQAWEZlMIuo0NBGRu4kCWETEEAWwiIghCmAREUMUwCIihiiARUQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAxRAIuIGKIAFhExRAEsImKI0T9Lfyfp6rs+5GPRNhtx39CmFpGBlArj4IYFy0+dHfLxmkf+bAK7EZHJQocgREQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAxRAIuIGKIAFhExRAEsImKIAlhExJAJD+Df/e53PPnkkyxdupSCggIOHToEwKuvvsrDDz9MUVERRUVFnD59OvScAwcO4Ha7yc/P58yZM6F6fX09+fn5uN1uqqurJ3oqIiJfy4R/HWV0dDRbtmxh3rx5dHd3s2LFChYtWgTAqlWrWL169YDxly5dwuPx4PF48Pl8lJWV8eabbwJQVVXFa6+9htPppKSkBJfLxQMPPDDRUxIRGZMJD2CHw4HD4QAgLi6O1NRUfD7fkOPr6uooKCggJiaGlJQU5syZQ1NTEwBz5swhJSUFgIKCAurq6hTAIjJpGP1C9ra2NrxeLxkZGfznf/4nhw8fpqamhvnz57Nlyxbi4+Px+XxkZGSEnuN0OkOBnZSUNKD+ZTAPp7e3F6/XO+IeA4EAwWCQnms9w4yyhn08GAzi/eTSiNc5GoFAYFTzmUjqbewiub9I7g0is7+5c+cOWjcWwNeuXWPDhg1s3bqVuLg4Vq5cybp167DZbLzyyivs3r2b559/ftzXGxsbO+TGGIzX68VutzP1nqnDjLIN+7jdbh/VOkfD6/WGbdlfl3obu0juL5J7g8jv71ZGzoK4fv06GzZsoLCwkLy8PABmzJhBdHQ0UVFRlJaWcv78eeDmnm17e3vouT6fD6fTOWRdRGSymPAAtiyLZ555htTUVMrKykJ1v98fun3y5EnS0tIAcLlceDwe+vr6aG1tpaWlhQULFpCenk5LSwutra309fXh8XhwuVwTPR0RkTGb8EMQH330EbW1tXzzm9+kqKgIgM2bN3P8+HEuXLgAQHJyMlVVVQCkpaWxZMkSli5dSnR0NJWVlURHRwNQWVnJU089RX9/PytWrAiFtojIZDDhAfztb3+b//7v/76tnpubO+Rz1q5dy9q1awd9znDPExGJZLoSTkTEEAWwiIghCmAREUMUwCIihiiARUQMUQCLiBiiABYRMUQBLCJiiAJYRMQQBbCIiCEKYBERQxTAIiKGKIBFRAxRAIuIGKIAFhExRAEsImKIAlhExBAFsIiIIQpgERFDFMAiIoYogEVEDFEAi4gYogAWETFEASwiYogCWETEELvpBu4WXX3XB61H22zEfUMvg8jdSO/8CXDDguWnzg76WM0jfzbB3YhIpNAhCBERQxTAIiKGKIBFRAzRMeAIMNQHdKAP6UTuZHpnGzbcB3SgD+lE7mQ6BCEiYsikD+D6+nry8/Nxu91UV1ebbicsuvquD/lv5v9LMd2eiIzRpD4E0d/fT1VVFa+99hpOp5OSkhJcLhcPPPCA6dbGzVcdojjycPoEdjO5dF8P0m9Zgz7mmHM/3deDOr4uRk3q/31NTU3MmTOHlJSbe4EFBQXU1dXdUQEsY9dvWRS/M/gPr55rPZxYtmiCO7pzDPfDDfTh8UjZLGuYrRjhfv3rX3PmzBmee+45AGpqamhqaqKysnLI55w7d47Y2NiJalFEBLvdTlpa2u11A70YlZmZaboFERFgkn8I53Q6aW9vD933+Xw4nU6DHYmIjNykDuD09HRaWlpobW2lr68Pj8eDy+Uy3ZaIyIhM6kMQdrudyspKnnrqKfr7+1mxYsWgx1lERCLRpP4QTkRkMpvUhyBERCYzBbCIiCEK4GGYvsz5d7/7HU8++SRLly6loKCAQ4cOAfDqq6/y8MMPU1RURFFREadPnw4958CBA7jdbvLz8zlz5kxY+3O5XBQWFlJUVMTy5csB6OzspKysjLy8PMrKyujq6gLAsix27tyJ2+2msLCQjz/+OKy9ffrpp6HtU1RUxJ//+Z/zk5/8xNi2q6ioICsri2XLloVqY9lWx44dIy8vj7y8PI4dOxbW/l544QUeffRRCgsLKS8v5+rVqwC0tbWxYMGC0Da89bz75uZmCgsLcbvd7Ny5k/E4wjlYb2N5HU2/nwdlyaCCwaC1ePFi67e//a3V29trFRYWWhcvXpzQHnw+n9Xc3GxZlmV98cUXVl5ennXx4kVr79691sGDB28bf/HiRauwsNDq7e21fvvb31qLFy+2gsFg2Pp75JFHrI6OjgG1F154wTpw4IBlWZZ14MAB68UXX7Qsy7JOnTplrV692rpx44Z19uxZq6SkJGx9/aFgMGj91V/9ldXW1mZs2zU2NlrNzc1WQUFBqDbabXXlyhXL5XJZV65csTo7Oy2Xy2V1dnaGrb8zZ85Y169ftyzLsl588cVQf62trQPG3WrFihXW2bNnrRs3blirV6+2Tp06FZbeRvs6RsL7eTDaAx7CrZc5x8TEhC5znkgOh4N58+YBEBcXR2pqKj6fb8jxdXV1FBQUEBMTQ0pKCnPmzKGpqWmi2g31UFxcDEBxcTEnT54cULfZbGRmZnL16lX8fv+E9PTee++RkpJCcnLysH2Hc9stXLiQ+Pj429Y5mm3V0NDAokWLSEhIID4+nkWLFo3bnvpg/WVnZ2O33zxRKjMzc8A594Px+/10d3eTmZmJzWajuLh4XN4zg/U2lKFex0h4Pw9GATwEn89HUlJS6L7T6Rw2/MKtra0Nr9dLRkYGAIcPH6awsJCKiorQr64mel69ejXLly/n3/7t3wDo6OjA4XAAMHPmTDo6OgbtLSkpacK2p8fjGfDra6Rsu9FuK5P/J3/+85+Tk5MTut/W1kZxcTFPPPEEH3744bB9h8toXsdIez9/SQE8CVy7do0NGzawdetW4uLiWLlyJW+99Ra1tbU4HA52795tpK+f/exnHDt2jH/6p3/i8OHDfPDBBwMet9ls2Gw2I719qa+vj7fffptHH30UIHQqVqMAAAPcSURBVGK23R+KhG01lP379xMdHc1jjz0G3PzN7J133qGmpoYtW7bwve99j+7u7gntKVJfx9FSAA8hUi5zvn79Ohs2bKCwsJC8vDwAZsyYQXR0NFFRUZSWlnL+/HkjPX+57MTERNxuN01NTSQmJoYOLfj9fqZPnz5ob+3t7ROyPevr65k3bx4zZswAImfbAaPeViZ6PHr0KKdOneJHP/pR6AdETEwM06ZNA2D+/PnMnj2bzz77bEJf49G+jpHyfv5DCuAhRMJlzpZl8cwzz5CamkpZWVmofuux05MnT4au/nO5XHg8Hvr6+mhtbaWlpYUFCxaEpbeenp7QXk9PTw+/+c1vSEtLw+VyUVNTA9z8drrFixeHequpqcGyLM6dO8e9994b+vU7nDweDwUFBaH7kbDtvjTabZWdnU1DQwNdXV10dXXR0NBAdnZ22Pqrr6/n4MGD7N+/nylTpoTqly9fpr+/HyC0rVJSUnA4HMTFxXHu3Dksyxowp/E22tcxEt7Pg5nUlyKHUyRc5vzRRx9RW1vLN7/5TYqKigDYvHkzx48f58KFCwAkJydTVVUFQFpaGkuWLGHp0qVER0dTWVlJdHR0WHrr6OigvLwcuPnF+MuWLSMnJ4f09HQ2btzIkSNHmDVrFnv27AEgNzeX06dP43a7mTJlCrt27QpLX7fq6enh3XffDW0fgJdeesnIttu8eTONjY1cuXKFnJwc1q9fz5o1a0a1rRISEli3bh0lJSUAlJeXk5CQELb+qqur6evrC/3wz8jIoKqqig8++IC9e/dit9uJiori2WefDfWxbds2KioqCAQC5OTkDDhuPJ69NTY2jvp1NP1+HowuRRYRMUSHIEREDFEAi4gYogAWETFEASwiYogCWETEEJ2GJvIH/vd//5ddu3Zx/vx57rvvPhITE9m6dStFRUWkpqbS29vLPffcw3e/+93Qt8CJjIUCWOQWlmXx9NNPU1xczI9//GMALly4QEdHB7Nnzw5dONHa2srTTz+NZVmsWLHCZMsyiekQhMgt3n//fex2OytXrgzV/vRP/3TAF7kApKSksGXLFn76059OdItyB1EAi9zi4sWLoa8A/Srz5s3j008/DXNHcidTAIuMkS4ila9LASxyi7S0tBH/uaT/+q//4k/+5E/C3JHcyRTAIrd46KGH6OvrC33BPNz8EO4P/xpEW1sbL774Ik888cREtyh3EH0Zj8gf8Pl87Nq1i48//pjY2FiSk5PZunUrjz32mE5Dk3GlABYRMUSHIEREDFEAi4gYogAWETFEASwiYogCWETEEAWwiIghCmAREUP+P1tmJpKE6NP9AAAAAElFTkSuQmCC%0A)\n",
    "\n",
    "In \\[50\\]:\n",
    "\n",
    "    text_len\n",
    "\n",
    "Out\\[50\\]:\n",
    "\n",
    "    0         5\n",
    "    1         5\n",
    "    2        34\n",
    "    3        22\n",
    "    4        13\n",
    "             ..\n",
    "    22680    60\n",
    "    22681    60\n",
    "    22682    60\n",
    "    22683    60\n",
    "    22684    60\n",
    "    Name: CD, Length: 22685, dtype: int64\n",
    "\n",
    "# BERT - Define Bert Tokenizer, Model def<a href=\"#BERT---Define-Bert-Tokenizer,-Model-def\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "Prep input data tensors, define BERT model for multi class\n",
    "classification useing pre-trained model from Huggingface BERT\n",
    "\n",
    "In \\[51\\]:\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                              do_lower_case=True)\n",
    "\n",
    "In \\[52\\]:\n",
    "\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type=='train'].CD.values, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        pad_to_max_length=True, \n",
    "        #max_length=256, \n",
    "        max_length=128,\n",
    "        #max_length=50, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type=='val'].CD.values, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        pad_to_max_length=True, \n",
    "        #max_length=256,\n",
    "        max_length=128,\n",
    "        #max_length=50, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "    Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
    "    Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
    "\n",
    "In \\[53\\]:\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "In \\[54\\]:\n",
    "\n",
    "    len(dataset_train), len(dataset_val)\n",
    "\n",
    "Out\\[54\\]:\n",
    "\n",
    "    (19282, 3403)\n",
    "\n",
    "In \\[55\\]:\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "    Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
    "    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
    "    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
    "    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "In \\[56\\]:\n",
    "\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "    batch_size = 3\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, \n",
    "                                  sampler=RandomSampler(dataset_train), \n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "    dataloader_validation = DataLoader(dataset_val, \n",
    "                                       sampler=SequentialSampler(dataset_val), \n",
    "                                       batch_size=batch_size)\n",
    "\n",
    "In \\[57\\]:\n",
    "\n",
    "    from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=1e-5, \n",
    "                      eps=1e-8)\n",
    "\n",
    "In \\[58\\]:\n",
    "\n",
    "    epochs = 5\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "In \\[64\\]:\n",
    "\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    def accuracy(preds, labels):\n",
    "        outputs = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(outputs == labels_flat)\n",
    "\n",
    "    def f1_score_func(preds, labels):\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "    def accuracy_per_class(preds, labels):\n",
    "        label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "        \n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "\n",
    "        for label in np.unique(labels_flat):\n",
    "            y_preds = preds_flat[labels_flat==label]\n",
    "            y_true = labels_flat[labels_flat==label]\n",
    "            print(f'Class: {label_dict_inverse[label]}')\n",
    "            print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "In \\[60\\]:\n",
    "\n",
    "    import random\n",
    "\n",
    "    seed_val = 17\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "In \\[61\\]:\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    print(device)\n",
    "\n",
    "    cuda\n",
    "\n",
    "In \\[62\\]:\n",
    "\n",
    "    def evaluate(dataloader_val):\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        loss_val_total = 0\n",
    "        predictions, true_vals = [], []\n",
    "        total_eval_accuracy = 0\n",
    "        \n",
    "        for batch in dataloader_val:\n",
    "            \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                     }\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss_val_total += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = inputs['labels'].cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(dataloader_val)\n",
    "      \n",
    "                \n",
    "        return loss_val_avg, predictions, true_vals, avg_val_accuracy\n",
    "                \n",
    "      \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "        import torch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# BERT Training<a href=\"#BERT-Training\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[67\\]:\n",
    "\n",
    "    #epochs = 4\n",
    "    #epochs = 5\n",
    "\n",
    "In \\[68\\]:\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                     }       \n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "             \n",
    "            \n",
    "        #torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "        torch.save(model.state_dict(), 'finetuned_BERT_epoch_{}.model'.format(epoch))\n",
    "\n",
    "            \n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        \n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        \n",
    "        val_loss, predictions, true_vals, avg_val_accuracy = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "    Epoch 1\n",
    "    Training loss: 0.63205881562842\n",
    "    Validation loss: 0.4508195354849395\n",
    "    F1 Score (Weighted): 0.8988236026877526\n",
    "    Accuracy: 0.9051395007342139\n",
    "\n",
    "    Epoch 2\n",
    "    Training loss: 0.2720410008467707\n",
    "    Validation loss: 0.3645775953438779\n",
    "    F1 Score (Weighted): 0.9294840590104655\n",
    "    Accuracy: 0.9318649045521289\n",
    "\n",
    "    Epoch 3\n",
    "    Training loss: 0.15605006541168706\n",
    "    Validation loss: 0.33522128653040745\n",
    "    F1 Score (Weighted): 0.9380992644763807\n",
    "    Accuracy: 0.9395007342143903\n",
    "\n",
    "    Epoch 4\n",
    "    Training loss: 0.09553318860604121\n",
    "    Validation loss: 0.3230386202712979\n",
    "    F1 Score (Weighted): 0.9449826509911491\n",
    "    Accuracy: 0.9462555066079296\n",
    "\n",
    "# Run only if GPU memory error above section. Manual Training<a href=\"#Run-only-if-GPU-memory--error-above-section.-Manual-Training\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "Run outside the epochs iteration only if needed\n",
    "\n",
    "In \\[66\\]:\n",
    "\n",
    "        val_loss, predictions, true_vals, avg_val_accuracy = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "    Validation loss: 0.9904041105922361\n",
    "    F1 Score (Weighted): 0.7650140247295772\n",
    "    Accuracy: 0.7964757709251095\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # upload external file before import\n",
    "    #from google.colab import files\n",
    "    #files.upload()\n",
    "    #import helper\n",
    "\n",
    "    #files.upload()\n",
    "    #import fc_model\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #epoch 1 \n",
    "    #torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    # download checkpoint file\n",
    "    #files.download('checkpoint.pth')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #torch.save(model.state_dict(), 'finetuned_BERT_epoch_{epoch}.model')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "        \n",
    "        torch.save(model.state_dict(), 'finetuned_BERT_epoch_{epoch1}.model')\n",
    "            \n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        \n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        \n",
    "        val_loss, predictions, true_vals, avg_val_accuracy  = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "    Epoch 1\n",
    "    Training loss: 0.4689319437316262\n",
    "    Validation loss: 0.07559893933844199\n",
    "    F1 Score (Weighted): 0.9885210834945934\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # epoch 2 manual run\n",
    "    for epoch in tqdm(range(2, 3)):\n",
    "        model.train()\n",
    "        \n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                     }       \n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "             \n",
    "            \n",
    "        #torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "        torch.save(model.state_dict(), 'finetuned_BERT_epoch_{}.model'.format(epoch))\n",
    "\n",
    "            \n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        \n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        \n",
    "        val_loss, predictions, true_vals, avg_val_accuracy  = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #epoch 2 \n",
    "    torch.save(model.state_dict(), 'checkpoint2.pth')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    epoch = 2\n",
    "    print('finetuned_BERT_epoch_{}.model'.format(epoch))\n",
    "\n",
    "    finetuned_BERT_epoch_2.model\n",
    "\n",
    "# Load the previously SAVED MODEL (Choose file here)<a href=\"#Load-the-previously-SAVED-MODEL-(Choose-file-here)\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "file for new prediction/ evaluation\n",
    "\n",
    "In \\[71\\]:\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
    "    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
    "    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
    "    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "Out\\[71\\]:\n",
    "\n",
    "    BertForSequenceClassification(\n",
    "      (bert): BertModel(\n",
    "        (embeddings): BertEmbeddings(\n",
    "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "          (position_embeddings): Embedding(512, 768)\n",
    "          (token_type_embeddings): Embedding(2, 768)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (encoder): BertEncoder(\n",
    "          (layer): ModuleList(\n",
    "            (0): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (1): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (2): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (3): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (4): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (5): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (6): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (7): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (8): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (9): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (10): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (11): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "        (pooler): BertPooler(\n",
    "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "          (activation): Tanh()\n",
    "        )\n",
    "      )\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "      (classifier): Linear(in_features=768, out_features=74, bias=True)\n",
    "    )\n",
    "\n",
    "In \\[72\\]:\n",
    "\n",
    "    #model.load_state_dict(torch.load('data_volume/finetuned_BERT_epoch_1.model', map_location=torch.device('cpu')))\n",
    "    model.load_state_dict(torch.load('finetuned_BERT_epoch_4.model', map_location=torch.device('cpu')))\n",
    "\n",
    "Out\\[72\\]:\n",
    "\n",
    "    <All keys matched successfully>\n",
    "\n",
    "In \\[73\\]:\n",
    "\n",
    "    avg_val_loss, predictions, true_vals, avg_val_accuracy  = evaluate(dataloader_validation)\n",
    "\n",
    "In \\[74\\]:\n",
    "\n",
    "    accuracy_per_class(predictions, true_vals)\n",
    "\n",
    "    Class: GRP_0\n",
    "    Accuracy: 542/596\n",
    "\n",
    "    Class: GRP_24\n",
    "    Accuracy: 39/43\n",
    "\n",
    "    Class: GRP_12\n",
    "    Accuracy: 27/39\n",
    "\n",
    "    Class: GRP_8\n",
    "    Accuracy: 81/99\n",
    "\n",
    "    Class: GRP_9\n",
    "    Accuracy: 22/38\n",
    "\n",
    "    Class: GRP_28\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_25\n",
    "    Accuracy: 35/38\n",
    "\n",
    "    Class: GRP_33\n",
    "    Accuracy: 31/38\n",
    "\n",
    "    Class: GRP_2\n",
    "    Accuracy: 29/38\n",
    "\n",
    "    Class: GRP_36\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_31\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_42\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_34\n",
    "    Accuracy: 35/37\n",
    "\n",
    "    Class: GRP_49\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_30\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_13\n",
    "    Accuracy: 33/37\n",
    "\n",
    "    Class: GRP_48\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_14\n",
    "    Accuracy: 29/37\n",
    "\n",
    "    Class: GRP_19\n",
    "    Accuracy: 22/38\n",
    "\n",
    "    Class: GRP_23\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_62\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_26\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_11\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_59\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_16\n",
    "    Accuracy: 34/37\n",
    "\n",
    "    Class: GRP_29\n",
    "    Accuracy: 37/38\n",
    "\n",
    "    Class: GRP_10\n",
    "    Accuracy: 35/38\n",
    "\n",
    "    Class: GRP_18\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_5\n",
    "    Accuracy: 34/38\n",
    "\n",
    "    Class: GRP_15\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_20\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_52\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_32\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_46\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_45\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_1\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_3\n",
    "    Accuracy: 24/37\n",
    "\n",
    "    Class: GRP_4\n",
    "    Accuracy: 36/38\n",
    "\n",
    "    Class: GRP_6\n",
    "    Accuracy: 36/37\n",
    "\n",
    "    Class: GRP_7\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_17\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_21\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_22\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_27\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_35\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_37\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_38\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_39\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_40\n",
    "    Accuracy: 35/38\n",
    "\n",
    "    Class: GRP_41\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_43\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_44\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_47\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_50\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_51\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_53\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_54\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_55\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_56\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_57\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_58\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_60\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_61\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_63\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_64\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_65\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_66\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_67\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_68\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_69\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_70\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_71\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_72\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_73\n",
    "    Accuracy: 37/37\n",
    "\n",
    "In \\[75\\]:\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "      Accuracy: 0.95\n",
    "      Validation Loss: 0.32"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
