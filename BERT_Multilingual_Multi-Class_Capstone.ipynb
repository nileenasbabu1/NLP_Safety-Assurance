{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Section<a href=\"#Init-Section\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow\n",
    "    tensorflow.__version__\n",
    "\n",
    "    # Initialize the random number generator\n",
    "    import random\n",
    "    random.seed(0)\n",
    "\n",
    "    # Ignore the warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    import io\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import spacy\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    #print(stop_words)\n",
    "\n",
    "    [nltk_data] Downloading package punkt to /root/nltk_data...\n",
    "    [nltk_data]   Unzipping tokenizers/punkt.zip.\n",
    "    [nltk_data] Downloading package stopwords to /root/nltk_data...\n",
    "    [nltk_data]   Unzipping corpora/stopwords.zip.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "\n",
    "    Mounted at /content/drive/\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import os\n",
    "    import sys\n",
    "    os.chdir('/content/drive/My Drive/AI Datasets/')\n",
    "\n",
    "# IMPORT LIBRARIES<a href=\"#IMPORT-LIBRARIES\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import glob\n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    import numpy as np\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.models import Sequential, load_model\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras import backend\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Embedding, LSTM, TimeDistributed, Flatten\n",
    "    from tensorflow.keras import metrics\n",
    "    import tensorflow as tf\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    !pip install transformers==3.0.0\n",
    "\n",
    "    Collecting transformers==3.0.0\n",
    "      Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
    "         |████████████████████████████████| 757kB 10.0MB/s \n",
    "    Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8)\n",
    "    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n",
    "    Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.8)\n",
    "    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.19.5)\n",
    "    Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n",
    "    Collecting tokenizers==0.8.0-rc4\n",
    "      Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
    "         |████████████████████████████████| 3.0MB 18.3MB/s \n",
    "    Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n",
    "    Collecting sacremoses\n",
    "      Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
    "         |████████████████████████████████| 890kB 24.2MB/s \n",
    "    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
    "    Collecting sentencepiece\n",
    "      Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
    "         |████████████████████████████████| 1.2MB 37.4MB/s \n",
    "    Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
    "    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
    "    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n",
    "    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
    "    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
    "    Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
    "    Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
    "    Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.0.0)\n",
    "    Building wheels for collected packages: sacremoses\n",
    "      Building wheel for sacremoses (setup.py) ... done\n",
    "      Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=31a0136d90cbeb445ba5e4d06ec833457d0e43d3581e091c5e091ae9c257a573\n",
    "      Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
    "    Successfully built sacremoses\n",
    "    Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
    "    Successfully installed sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.8.0rc4 transformers-3.0.0\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import torch\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    from transformers import BertTokenizer\n",
    "    from torch.utils.data import TensorDataset\n",
    "\n",
    "    from transformers import BertForSequenceClassification\n",
    "\n",
    "# OPTIONAL - BERT models For refence only<a href=\"#OPTIONAL---BERT-models-For-refence-only\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "    map_name_to_handle = {\n",
    "        'bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "        'bert_en_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "        'bert_multi_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "        'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "        'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "        'albert_en_base':\n",
    "            'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "        'electra_small':\n",
    "            'https://tfhub.dev/google/electra_small/2',\n",
    "        'electra_base':\n",
    "            'https://tfhub.dev/google/electra_base/2',\n",
    "        'experts_pubmed':\n",
    "            'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "        'experts_wiki_books':\n",
    "            'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "        'talking-heads_base':\n",
    "            'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "    }\n",
    "\n",
    "    map_model_to_preprocess = {\n",
    "        'bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'bert_en_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'bert_multi_cased_L-12_H-768_A-12':\n",
    "            'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/2',\n",
    "        'albert_en_base':\n",
    "            'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
    "        'electra_small':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'electra_base':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'experts_pubmed':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'experts_wiki_books':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "        'talking-heads_base':\n",
    "            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',\n",
    "    }\n",
    "\n",
    "    tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "    tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "    print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "    print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n",
    "\n",
    "    BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
    "    Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\n",
    "\n",
    "# OPTIONAL - Load dataset afresh - Run only needed<a href=\"#OPTIONAL---Load-dataset-afresh---Run-only-needed\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import unicodedata\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # Converts the unicode file to ascii\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s))\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # Read the data as a data frame\n",
    "    import sys\n",
    "    dataframe = pd.read_excel('input_data.xlsx')\n",
    "    dataframe = dataframe.fillna(method=\"ffill\") # Deal with N/A\n",
    "    dataframe.head()\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "|     | Short description             | Description                                       | Caller            | Assignment group |\n",
    "|-----|-------------------------------|---------------------------------------------------|-------------------|------------------|\n",
    "| 0   | login issue                   | -verified user details.(employee# & manager na... | spxjnwir pjlcoqds | GRP_0            |\n",
    "| 1   | outlook                       | \\r\\n\\r\\nreceived from: hmjdrvpb.komuaywn@gmail... | hmjdrvpb komuaywn | GRP_0            |\n",
    "| 2   | cant log in to vpn            | \\r\\n\\r\\nreceived from: eylqgodm.ybqkwiam@gmail... | eylqgodm ybqkwiam | GRP_0            |\n",
    "| 3   | unable to access hr_tool page | unable to access hr_tool page                     | xbkucsvz gcpydteq | GRP_0            |\n",
    "| 4   | skype error                   | skype error                                       | owlgqjme qhcozdfx | GRP_0            |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    def preprocess_row(w):\n",
    "        w = unicode_to_ascii(w)\n",
    "        return w\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    for k in range(0, dataframe.shape[0]):\n",
    "      dataframe.loc[k, ['PP Short description', 'PP Description']] = [preprocess_row(str(dataframe['Short description'][k])), preprocess_row(str(dataframe['Description'][k]))]\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataframe['Grp No'] = dataframe['Assignment group'].str.extract(r'_{1}(\\d+)', expand=False) \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    C_unique_grp = pd.DataFrame(dataframe['Caller'].value_counts())\n",
    "    C_unique_grp.reset_index(level=0, inplace=True)   \n",
    "    C_unique_grp.rename(columns={\"index\": \"Caller\", \"Caller\": \"Counts\"}, inplace=True)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    C_unique_grp['Caller'] = C_unique_grp['Caller'].str.lower()\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    for j in range(0, C_unique_grp.shape[0]):\n",
    "      C_unique_grp.loc[j, ['Caller Encoded', 'part email id']] = ['Caller'+str(j), '.'.join(str(C_unique_grp['Caller'][j]).split())]\n",
    "    C_unique_grp.info()\n",
    "\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 2950 entries, 0 to 2949\n",
    "    Data columns (total 4 columns):\n",
    "     #   Column          Non-Null Count  Dtype \n",
    "    ---  ------          --------------  ----- \n",
    "     0   Caller          2950 non-null   object\n",
    "     1   Counts          2950 non-null   int64 \n",
    "     2   Caller Encoded  2950 non-null   object\n",
    "     3   part email id   2950 non-null   object\n",
    "    dtypes: int64(1), object(3)\n",
    "    memory usage: 92.3+ KB\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    C_unique_grp.head(50)\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "|     | Caller            | Counts | Caller Encoded | part email id     |\n",
    "|-----|-------------------|--------|----------------|-------------------|\n",
    "| 0   | bpctwhsn kzqsbmtp | 810    | Caller0        | bpctwhsn.kzqsbmtp |\n",
    "| 1   | zkbogxib qsejzdzo | 151    | Caller1        | zkbogxib.qsejzdzo |\n",
    "| 2   | fumkcsji sarmtlhy | 134    | Caller2        | fumkcsji.sarmtlhy |\n",
    "| 3   | rbozivdq gmlhrtvp | 87     | Caller3        | rbozivdq.gmlhrtvp |\n",
    "| 4   | rkupnshb gsmzfojw | 71     | Caller4        | rkupnshb.gsmzfojw |\n",
    "| 5   | jloygrwh acvztedi | 64     | Caller5        | jloygrwh.acvztedi |\n",
    "| 6   | spxqmiry zpwgoqju | 63     | Caller6        | spxqmiry.zpwgoqju |\n",
    "| 7   | oldrctiu bxurpsyi | 57     | Caller7        | oldrctiu.bxurpsyi |\n",
    "| 8   | olckhmvx pcqobjnd | 54     | Caller8        | olckhmvx.pcqobjnd |\n",
    "| 9   | dkmcfreg anwmfvlg | 51     | Caller9        | dkmcfreg.anwmfvlg |\n",
    "| 10  | jyoqwxhz clhxsoqy | 51     | Caller10       | jyoqwxhz.clhxsoqy |\n",
    "| 11  | efbwiadp dicafxhv | 45     | Caller11       | efbwiadp.dicafxhv |\n",
    "| 12  | afkstcev utbnkyop | 32     | Caller12       | afkstcev.utbnkyop |\n",
    "| 13  | gzhapcld fdigznbk | 30     | Caller13       | gzhapcld.fdigznbk |\n",
    "| 14  | mnlazfsr mtqrkhnx | 28     | Caller14       | mnlazfsr.mtqrkhnx |\n",
    "| 15  | uvrbhlnt bjrmalzi | 27     | Caller15       | uvrbhlnt.bjrmalzi |\n",
    "| 16  | entuakhp xrnhtdmk | 25     | Caller16       | entuakhp.xrnhtdmk |\n",
    "| 17  | jionmpsf wnkpzcmv | 24     | Caller17       | jionmpsf.wnkpzcmv |\n",
    "| 18  | vzqomdgt jwoqbuml | 24     | Caller18       | vzqomdgt.jwoqbuml |\n",
    "| 19  | bozdftwx smylqejw | 23     | Caller19       | bozdftwx.smylqejw |\n",
    "| 20  | rxoynvgi ntgdsehl | 21     | Caller20       | rxoynvgi.ntgdsehl |\n",
    "| 21  | utyeofsk rdyzpwhi | 21     | Caller21       | utyeofsk.rdyzpwhi |\n",
    "| 22  | qasdhyzm yuglsrwx | 21     | Caller22       | qasdhyzm.yuglsrwx |\n",
    "| 23  | vbwszcqn nlbqsuyv | 19     | Caller23       | vbwszcqn.nlbqsuyv |\n",
    "| 24  | hbmwlprq ilfvyodx | 17     | Caller24       | hbmwlprq.ilfvyodx |\n",
    "| 25  | ugyothfz ugrmkdhx | 17     | Caller25       | ugyothfz.ugrmkdhx |\n",
    "| 26  | niptbwdq csenjruz | 16     | Caller26       | niptbwdq.csenjruz |\n",
    "| 27  | pfzxecbo ptygkvzl | 16     | Caller27       | pfzxecbo.ptygkvzl |\n",
    "| 28  | vfrdxtqw jfbmsenz | 16     | Caller28       | vfrdxtqw.jfbmsenz |\n",
    "| 29  | ughzilfm cfibdamq | 16     | Caller29       | ughzilfm.cfibdamq |\n",
    "| 30  | mfeyouli ndobtzpw | 15     | Caller30       | mfeyouli.ndobtzpw |\n",
    "| 31  | uxgrdjfc kqxdjeov | 15     | Caller31       | uxgrdjfc.kqxdjeov |\n",
    "| 32  | xwirzvda okhyipgr | 14     | Caller32       | xwirzvda.okhyipgr |\n",
    "| 33  | ctvaejbo mjcerqwo | 14     | Caller33       | ctvaejbo.mjcerqwo |\n",
    "| 34  | tqfnalpj qyoscnge | 13     | Caller34       | tqfnalpj.qyoscnge |\n",
    "| 35  | wktesmbp lorjymef | 13     | Caller35       | wktesmbp.lorjymef |\n",
    "| 36  | aorthyme rnsuipbk | 13     | Caller36       | aorthyme.rnsuipbk |\n",
    "| 37  | obanjrhg rnafleys | 13     | Caller37       | obanjrhg.rnafleys |\n",
    "| 38  | ayrhcfxi zartupsw | 12     | Caller38       | ayrhcfxi.zartupsw |\n",
    "| 39  | qcehailo wqynckxg | 12     | Caller39       | qcehailo.wqynckxg |\n",
    "| 40  | qcfmxgid jvxanwre | 12     | Caller40       | qcfmxgid.jvxanwre |\n",
    "| 41  | ijplstng juybetlo | 12     | Caller41       | ijplstng.juybetlo |\n",
    "| 42  | qftpazns fxpnytmk | 12     | Caller42       | qftpazns.fxpnytmk |\n",
    "| 43  | kbnfxpsy gehxzayq | 12     | Caller43       | kbnfxpsy.gehxzayq |\n",
    "| 44  | zuxcfonv nyhpkrbe | 12     | Caller44       | zuxcfonv.nyhpkrbe |\n",
    "| 45  | rdfjsawg zpmxgdcw | 12     | Caller45       | rdfjsawg.zpmxgdcw |\n",
    "| 46  | vkzwafuh tcjnuswg | 12     | Caller46       | vkzwafuh.tcjnuswg |\n",
    "| 47  | wvngzrca sfmrzdth | 12     | Caller47       | wvngzrca.sfmrzdth |\n",
    "| 48  | lwgytuxq qspdztiw | 12     | Caller48       | lwgytuxq.qspdztiw |\n",
    "| 49  | oetlgbfw bsctrnwp | 11     | Caller49       | oetlgbfw.bsctrnwp |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    for i in range(0, dataframe.shape[0]):\n",
    "    #for i in range(0, 100):\n",
    "      j=0\n",
    "      v_found = 0\n",
    "      for j in range(0, C_unique_grp.shape[0]):\n",
    "        if (str(C_unique_grp['Caller'][j]) in str(dataframe['PP Short description'][i])):\n",
    "          dataframe['PP Short description'][i] = str(dataframe['PP Short description'][i]).replace(str(C_unique_grp['Caller'][j]), str(C_unique_grp['Caller Encoded'][j]))\n",
    "          v_found = 1\n",
    "        \n",
    "        if (str(C_unique_grp['part email id'][j]) in str(dataframe['PP Short description'][i])):\n",
    "          dataframe['PP Short description'][i] = str(dataframe['PP Short description'][i]).replace(str(C_unique_grp['part email id'][j]), str(C_unique_grp['Caller Encoded'][j]))\n",
    "          v_found = 1\n",
    "        \n",
    "        if (str(C_unique_grp['Caller'][j]) in str(dataframe['PP Description'][i])):\n",
    "          dataframe['PP Description'][i] = str(dataframe['PP Description'][i]).replace(str(C_unique_grp['Caller'][j]), str(C_unique_grp['Caller Encoded'][j]))\n",
    "          v_found = 1\n",
    "        \n",
    "        if (str(C_unique_grp['part email id'][j]) in str(dataframe['PP Description'][i])):\n",
    "          dataframe['PP Description'][i] = str(dataframe['PP Description'][i]).replace(str(C_unique_grp['part email id'][j]), str(C_unique_grp['Caller Encoded'][j]))\n",
    "          v_found = 1\n",
    "        \n",
    "        if v_found == 1:\n",
    "          break\n",
    "       \n",
    "\n",
    "    ---------------------------------------------------------------------------\n",
    "    KeyboardInterrupt                         Traceback (most recent call last)\n",
    "    <ipython-input-19-0773618e3331> in <module>()\n",
    "          8       v_found = 1\n",
    "          9 \n",
    "    ---> 10     if (str(C_unique_grp['part email id'][j]) in str(dataframe['PP Short description'][i])):\n",
    "         11       dataframe['PP Short description'][i] = str(dataframe['PP Short description'][i]).replace(str(C_unique_grp['part email id'][j]), str(C_unique_grp['Caller Encoded'][j]))\n",
    "         12       v_found = 1\n",
    "\n",
    "    /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in __getitem__(self, key)\n",
    "       2876                 if self.columns.nlevels > 1:\n",
    "       2877                     return self._getitem_multilevel(key)\n",
    "    -> 2878                 return self._get_item_cache(key)\n",
    "       2879 \n",
    "       2880         # Do we have a slicer (on rows)?\n",
    "\n",
    "    /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py in _get_item_cache(self, item)\n",
    "       3535         \"\"\"Return the cached item, item represents a label indexer.\"\"\"\n",
    "       3536         cache = self._item_cache\n",
    "    -> 3537         res = cache.get(item)\n",
    "       3538         if res is None:\n",
    "       3539             # All places that call _get_item_cache have unique columns,\n",
    "\n",
    "    KeyboardInterrupt: \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataframe.head(200)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataframe['Caller'] = dataframe['Caller'].str.lower()\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataframe = pd.merge(dataframe, C_unique_grp[['Caller', 'Caller Encoded']], on='Caller', how='left')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataframe\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    pip install polyglot\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    pip install pyicu\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    pip install pycld2\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    pip install morfessor\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataframe['Language'] = ' '\n",
    "    dataframe['Language Code'] = ' '\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import polyglot\n",
    "    from polyglot.text import Text, Word\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    text = Text(\"Bonjour, Mesdames.\")\n",
    "    print(\"Language Detected: Code={}, Name={}\\n\".format(text.language.code, text.language.name))\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    for i in range(0, dataframe.shape[0]):\n",
    "      #stripNonUtf8(str(dataframe['Short description'][i]))\n",
    "      try:\n",
    "        text = Text(str(dataframe['PP Short description'][i]) + ' ' + str(dataframe['PP Description'][i]))\n",
    "        dataframe.loc[i, ['Language', 'Language Code']] = [text.language.name, text.language.code]\n",
    "      except:\n",
    "        dataframe.loc[i, ['Language', 'Language Code']] = ['  ', '  ']\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataframe\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    L_unique_grp = pd.DataFrame(dataframe['Language'].value_counts())\n",
    "    L_unique_grp.reset_index(level=0, inplace=True)   \n",
    "    L_unique_grp.rename(columns={\"index\": \"Language\", \"Language\": \"Counts\"}, inplace=True)\n",
    "    L_unique_grp.info()\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    L_unique_grp\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataframe.info()\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #Save Pre-Processed datataframe\n",
    "    #dataframe\n",
    "    pd.DataFrame(dataframe).to_csv('dataframe_pp_20210127.csv',index=True)\n",
    "\n",
    "# Load Pre-Processed dataset<a href=\"#Load-Pre-Processed-dataset\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # Read the data as a data frame\n",
    "\n",
    "    dataframe = pd.read_csv('dataframe_pp_20210127.csv')\n",
    "    dataframe.head()\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "|     | Unnamed: 0 | Short description             | Description                                       | Caller            | Assignment group | PP Short description          | PP Description                                    | Grp No | Caller Encoded | Language | Language Code |\n",
    "|-----|------------|-------------------------------|---------------------------------------------------|-------------------|------------------|-------------------------------|---------------------------------------------------|--------|----------------|----------|---------------|\n",
    "| 0   | 0          | login issue                   | -verified user details.(employee# & manager na... | spxjnwir pjlcoqds | GRP_0            | login issue                   | -verified user details.(employee# & manager na... | 0      | Caller1835     | English  | en            |\n",
    "| 1   | 1          | outlook                       | \\r\\n\\r\\nreceived from: hmjdrvpb.komuaywn@gmail... | hmjdrvpb komuaywn | GRP_0            | outlook                       | \\r\\n\\r\\nreceived from: Caller434@gmail.com\\r\\n... | 0      | Caller434      | English  | en            |\n",
    "| 2   | 2          | cant log in to vpn            | \\r\\n\\r\\nreceived from: eylqgodm.ybqkwiam@gmail... | eylqgodm ybqkwiam | GRP_0            | cant log in to vpn            | \\r\\n\\r\\nreceived from: Caller371@gmail.com\\r\\n... | 0      | Caller371      | English  | en            |\n",
    "| 3   | 3          | unable to access hr_tool page | unable to access hr_tool page                     | xbkucsvz gcpydteq | GRP_0            | unable to access hr_tool page | unable to access hr_tool page                     | 0      | Caller597      | English  | en            |\n",
    "| 4   | 4          | skype error                   | skype error                                       | owlgqjme qhcozdfx | GRP_0            | skype error                   | skype error                                       | 0      | Caller230      | Latin    | la            |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    y_counts = pd.DataFrame(dataframe['Grp No'].value_counts())\n",
    "    p = y_counts.index.values\n",
    "    y_counts.insert( 0, column=\"new\",value = p)\n",
    "    y_counts.columns = ['Grp No', 'counts']\n",
    "    y_counts['Grp No'].astype(int)\n",
    "    y_counts['counts'].astype(int)\n",
    "    y_counts\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "|     | Grp No | counts |\n",
    "|-----|--------|--------|\n",
    "| 0   | 0      | 3976   |\n",
    "| 8   | 8      | 661    |\n",
    "| 24  | 24     | 289    |\n",
    "| 12  | 12     | 257    |\n",
    "| 9   | 9      | 252    |\n",
    "| ... | ...    | ...    |\n",
    "| 61  | 61     | 1      |\n",
    "| 67  | 67     | 1      |\n",
    "| 35  | 35     | 1      |\n",
    "| 70  | 70     | 1      |\n",
    "| 73  | 73     | 1      |\n",
    "\n",
    "74 rows × 2 columns\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    y_counts.head(50)\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "|     | Grp No | counts |\n",
    "|-----|--------|--------|\n",
    "| 0   | 0      | 3976   |\n",
    "| 8   | 8      | 661    |\n",
    "| 24  | 24     | 289    |\n",
    "| 12  | 12     | 257    |\n",
    "| 9   | 9      | 252    |\n",
    "| 2   | 2      | 241    |\n",
    "| 19  | 19     | 215    |\n",
    "| 3   | 3      | 200    |\n",
    "| 6   | 6      | 184    |\n",
    "| 13  | 13     | 145    |\n",
    "| 10  | 10     | 140    |\n",
    "| 5   | 5      | 129    |\n",
    "| 14  | 14     | 118    |\n",
    "| 25  | 25     | 116    |\n",
    "| 33  | 33     | 107    |\n",
    "| 4   | 4      | 100    |\n",
    "| 29  | 29     | 97     |\n",
    "| 18  | 18     | 88     |\n",
    "| 16  | 16     | 85     |\n",
    "| 17  | 17     | 81     |\n",
    "| 31  | 31     | 69     |\n",
    "| 7   | 7      | 68     |\n",
    "| 34  | 34     | 62     |\n",
    "| 26  | 26     | 56     |\n",
    "| 40  | 40     | 45     |\n",
    "| 28  | 28     | 44     |\n",
    "| 41  | 41     | 40     |\n",
    "| 30  | 30     | 39     |\n",
    "| 15  | 15     | 39     |\n",
    "| 42  | 42     | 37     |\n",
    "| 20  | 20     | 36     |\n",
    "| 45  | 45     | 35     |\n",
    "| 22  | 22     | 31     |\n",
    "| 1   | 1      | 31     |\n",
    "| 11  | 11     | 30     |\n",
    "| 21  | 21     | 29     |\n",
    "| 47  | 47     | 27     |\n",
    "| 48  | 48     | 25     |\n",
    "| 23  | 23     | 25     |\n",
    "| 62  | 62     | 25     |\n",
    "| 60  | 60     | 20     |\n",
    "| 39  | 39     | 19     |\n",
    "| 27  | 27     | 18     |\n",
    "| 37  | 37     | 16     |\n",
    "| 36  | 36     | 15     |\n",
    "| 44  | 44     | 15     |\n",
    "| 50  | 50     | 14     |\n",
    "| 65  | 65     | 11     |\n",
    "| 53  | 53     | 11     |\n",
    "| 52  | 52     | 9      |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #incidentsData_Group_0 = dataframe2[dataframe2['Assignment group'] == 'GRP_0']\n",
    "    #incidentsData_Others = dataframe2[dataframe2['Assignment group'] != 'GRP_0']\n",
    "    #max_incident_cnt = dataframe2['Assignment group'].value_counts().max()\n",
    "\n",
    "    no_upsampling_grp = ['GRP_0', 'GRP_8', 'GRP_24', 'GRP_12', 'GRP_9']\n",
    "    incidentsData_no_upsample = dataframe[dataframe['Assignment group'].isin(no_upsampling_grp)]\n",
    "    incidentsData_Others = dataframe[~dataframe['Assignment group'].isin(no_upsampling_grp)]\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #incidentsData_Others.shape\n",
    "    incidentsData_no_upsample.shape\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    (5435, 11)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # Treat the imbalance in the 'other' dataset by resampling\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "    incidentsData_upsampled = incidentsData_Others[0:0]\n",
    "\n",
    "    # Upsample minority class\n",
    "    for grp in incidentsData_Others['Assignment group'].unique():\n",
    "        incidentsData_Group = incidentsData_Others[incidentsData_Others['Assignment group'] == grp]\n",
    "        resampled = resample(incidentsData_Group, \n",
    "                             replace=True, # sample with replacement\n",
    "                             #n_samples=int(max_incident_cnt/2), \n",
    "                             n_samples=int(250), \n",
    "                             random_state=123) # reproducible results\n",
    "        \n",
    "        incidentsData_upsampled = incidentsData_upsampled.append(resampled)\n",
    "\n",
    "    incidentsData_Others_upsample = pd.concat([incidentsData_no_upsample,incidentsData_upsampled])\n",
    "    incidentsData_Others_upsample.reset_index(inplace=True)\n",
    "\n",
    "    descending_order = incidentsData_upsampled['Assignment group'].value_counts().sort_values(ascending=False).index\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    from wordcloud import WordCloud, STOPWORDS \n",
    "    import seaborn as sns\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    flatui = ['#2E82A8','#00A0B8','#00BDB4','#53D69F','#A5EB84','#F9F871']\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    plt.subplots(figsize=(22,5))\n",
    "    #add code to rotate the labels\n",
    "    ax=sns.countplot(x='Assignment group', data=incidentsData_upsampled, palette = sns.color_palette(flatui),order=descending_order)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "![](data:image/png;base64,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%0A)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    incidentsData_Others_upsample.shape\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    (22685, 12)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    possible_labels = incidentsData_Others_upsample['Assignment group'].unique()\n",
    "\n",
    "    label_dict = {}\n",
    "    for index, possible_label in enumerate(possible_labels):\n",
    "        label_dict[possible_label] = index\n",
    "    label_dict\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    {'GRP_0': 0,\n",
    "     'GRP_1': 5,\n",
    "     'GRP_10': 11,\n",
    "     'GRP_11': 12,\n",
    "     'GRP_12': 3,\n",
    "     'GRP_13': 13,\n",
    "     'GRP_14': 14,\n",
    "     'GRP_15': 15,\n",
    "     'GRP_16': 16,\n",
    "     'GRP_17': 17,\n",
    "     'GRP_18': 18,\n",
    "     'GRP_19': 19,\n",
    "     'GRP_2': 20,\n",
    "     'GRP_20': 21,\n",
    "     'GRP_21': 22,\n",
    "     'GRP_22': 23,\n",
    "     'GRP_23': 24,\n",
    "     'GRP_24': 4,\n",
    "     'GRP_25': 25,\n",
    "     'GRP_26': 26,\n",
    "     'GRP_27': 27,\n",
    "     'GRP_28': 28,\n",
    "     'GRP_29': 29,\n",
    "     'GRP_3': 6,\n",
    "     'GRP_30': 30,\n",
    "     'GRP_31': 31,\n",
    "     'GRP_32': 61,\n",
    "     'GRP_33': 32,\n",
    "     'GRP_34': 33,\n",
    "     'GRP_35': 34,\n",
    "     'GRP_36': 35,\n",
    "     'GRP_37': 36,\n",
    "     'GRP_38': 37,\n",
    "     'GRP_39': 38,\n",
    "     'GRP_4': 7,\n",
    "     'GRP_40': 39,\n",
    "     'GRP_41': 40,\n",
    "     'GRP_42': 41,\n",
    "     'GRP_43': 42,\n",
    "     'GRP_44': 43,\n",
    "     'GRP_45': 44,\n",
    "     'GRP_46': 45,\n",
    "     'GRP_47': 46,\n",
    "     'GRP_48': 47,\n",
    "     'GRP_49': 48,\n",
    "     'GRP_5': 8,\n",
    "     'GRP_50': 49,\n",
    "     'GRP_51': 50,\n",
    "     'GRP_52': 51,\n",
    "     'GRP_53': 52,\n",
    "     'GRP_54': 53,\n",
    "     'GRP_55': 54,\n",
    "     'GRP_56': 55,\n",
    "     'GRP_57': 56,\n",
    "     'GRP_58': 57,\n",
    "     'GRP_59': 58,\n",
    "     'GRP_6': 9,\n",
    "     'GRP_60': 59,\n",
    "     'GRP_61': 60,\n",
    "     'GRP_62': 62,\n",
    "     'GRP_63': 63,\n",
    "     'GRP_64': 64,\n",
    "     'GRP_65': 65,\n",
    "     'GRP_66': 66,\n",
    "     'GRP_67': 67,\n",
    "     'GRP_68': 68,\n",
    "     'GRP_69': 69,\n",
    "     'GRP_7': 10,\n",
    "     'GRP_70': 70,\n",
    "     'GRP_71': 71,\n",
    "     'GRP_72': 72,\n",
    "     'GRP_73': 73,\n",
    "     'GRP_8': 1,\n",
    "     'GRP_9': 2}\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    df = incidentsData_Others_upsample.copy()\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    df['label'] = df['Assignment group'].replace(label_dict)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    df.head()\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "|     | index | Unnamed: 0 | Short description             | Description                                       | Caller            | Assignment group | PP Short description          | PP Description                                    | Grp No | Caller Encoded | Language | Language Code | label |\n",
    "|-----|-------|------------|-------------------------------|---------------------------------------------------|-------------------|------------------|-------------------------------|---------------------------------------------------|--------|----------------|----------|---------------|-------|\n",
    "| 0   | 0     | 0          | login issue                   | -verified user details.(employee# & manager na... | spxjnwir pjlcoqds | GRP_0            | login issue                   | -verified user details.(employee# & manager na... | 0      | Caller1835     | English  | en            | 0     |\n",
    "| 1   | 1     | 1          | outlook                       | \\r\\n\\r\\nreceived from: hmjdrvpb.komuaywn@gmail... | hmjdrvpb komuaywn | GRP_0            | outlook                       | \\r\\n\\r\\nreceived from: Caller434@gmail.com\\r\\n... | 0      | Caller434      | English  | en            | 0     |\n",
    "| 2   | 2     | 2          | cant log in to vpn            | \\r\\n\\r\\nreceived from: eylqgodm.ybqkwiam@gmail... | eylqgodm ybqkwiam | GRP_0            | cant log in to vpn            | \\r\\n\\r\\nreceived from: Caller371@gmail.com\\r\\n... | 0      | Caller371      | English  | en            | 0     |\n",
    "| 3   | 3     | 3          | unable to access hr_tool page | unable to access hr_tool page                     | xbkucsvz gcpydteq | GRP_0            | unable to access hr_tool page | unable to access hr_tool page                     | 0      | Caller597      | English  | en            | 0     |\n",
    "| 4   | 4     | 4          | skype error                   | skype error                                       | owlgqjme qhcozdfx | GRP_0            | skype error                   | skype error                                       | 0      | Caller230      | Latin    | la            | 0     |\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                      df.label.values, \n",
    "                                                      test_size=0.15, \n",
    "                                                      random_state=42, \n",
    "                                                      stratify=df.label.values)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "    df.loc[X_train, 'data_type'] = 'train'\n",
    "    df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    df.groupby(['Assignment group', 'label', 'data_type']).count()\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "|                  |       |           | index | Unnamed: 0 | Short description | Description | Caller | PP Short description | PP Description | Grp No | Caller Encoded | Language | Language Code |\n",
    "|------------------|-------|-----------|-------|------------|-------------------|-------------|--------|----------------------|----------------|--------|----------------|----------|---------------|\n",
    "| Assignment group | label | data_type |       |            |                   |             |        |                      |                |        |                |          |               |\n",
    "| GRP_0            | 0     | train     | 3380  | 3380       | 3380              | 3380        | 3380   | 3380                 | 3380           | 3380   | 3380           | 3380     | 3380          |\n",
    "|                  |       | val       | 596   | 596        | 596               | 596         | 596    | 596                  | 596            | 596    | 596            | 596      | 596           |\n",
    "| GRP_1            | 5     | train     | 213   | 213        | 213               | 213         | 213    | 213                  | 213            | 213    | 213            | 213      | 213           |\n",
    "|                  |       | val       | 37    | 37         | 37                | 37          | 37     | 37                   | 37             | 37     | 37             | 37       | 37            |\n",
    "| GRP_10           | 11    | train     | 212   | 212        | 212               | 212         | 212    | 212                  | 212            | 212    | 212            | 212      | 212           |\n",
    "| ...              | ...   | ...       | ...   | ...        | ...               | ...         | ...    | ...                  | ...            | ...    | ...            | ...      | ...           |\n",
    "| GRP_73           | 73    | val       | 37    | 37         | 37                | 37          | 37     | 37                   | 37             | 37     | 37             | 37       | 37            |\n",
    "| GRP_8            | 1     | train     | 562   | 562        | 562               | 562         | 562    | 562                  | 562            | 562    | 562            | 562      | 562           |\n",
    "|                  |       | val       | 99    | 99         | 99                | 99          | 99     | 99                   | 99             | 99     | 99             | 99       | 99            |\n",
    "| GRP_9            | 2     | train     | 214   | 214        | 214               | 214         | 214    | 214                  | 214            | 214    | 214            | 214      | 214           |\n",
    "|                  |       | val       | 38    | 38         | 38                | 38          | 38     | 38                   | 38             | 38     | 38             | 38       | 38            |\n",
    "\n",
    "148 rows × 11 columns\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #df['CD'] = df['PP Short description'] + ' . ' + df['PP Description']\n",
    "    df['CD'] = np.where(df['PP Short description'].str.strip()==df['PP Description'].str.strip(),\n",
    "                                                    df['PP Short description'].str.strip(),\n",
    "                                                    df['PP Short description'].str.strip() + \" . \" + df['PP Description'].str.strip())\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    fig = plt.figure(figsize=(6,9))\n",
    "    text_len=df['CD'].str.split().map(lambda x: len(str(x).split(\" \")))\n",
    "    sns.displot(text_len.dropna(),color='#00A0B8',binwidth=50)\n",
    "    fig.suptitle('Words in description')\n",
    "    plt.show()\n",
    "\n",
    "    <Figure size 432x648 with 0 Axes>\n",
    "\n",
    "![](data:image/png;base64,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%0A)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    text_len\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    0        36\n",
    "    1        27\n",
    "    2        17\n",
    "    3         5\n",
    "    4         2\n",
    "             ..\n",
    "    22680    60\n",
    "    22681    60\n",
    "    22682    60\n",
    "    22683    60\n",
    "    22684    60\n",
    "    Name: CD, Length: 22685, dtype: int64\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    df['XCombined'] = df['Caller Encoded'] + ' . ' + df['Language'] + ' . ' + df['CD'] \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    df.info()\n",
    "\n",
    "    <class 'pandas.core.frame.DataFrame'>\n",
    "    RangeIndex: 22685 entries, 0 to 22684\n",
    "    Data columns (total 16 columns):\n",
    "     #   Column                Non-Null Count  Dtype \n",
    "    ---  ------                --------------  ----- \n",
    "     0   index                 22685 non-null  int64 \n",
    "     1   Unnamed: 0            22685 non-null  int64 \n",
    "     2   Short description     22685 non-null  object\n",
    "     3   Description           22685 non-null  object\n",
    "     4   Caller                22685 non-null  object\n",
    "     5   Assignment group      22685 non-null  object\n",
    "     6   PP Short description  22685 non-null  object\n",
    "     7   PP Description        22685 non-null  object\n",
    "     8   Grp No                22685 non-null  int64 \n",
    "     9   Caller Encoded        22685 non-null  object\n",
    "     10  Language              22685 non-null  object\n",
    "     11  Language Code         22685 non-null  object\n",
    "     12  label                 22685 non-null  int64 \n",
    "     13  data_type             22685 non-null  object\n",
    "     14  CD                    22685 non-null  object\n",
    "     15  XCombined             22685 non-null  object\n",
    "    dtypes: int64(4), object(12)\n",
    "    memory usage: 2.8+ MB\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #df[df.data_type=='train'] ['X Combined'].values\n",
    "    df[df.data_type=='val'].XCombined.values\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    array(['Caller434 . English . outlook . received from: Caller434@gmail.com\\r\\n\\r\\nhello team,\\r\\n\\r\\nmy meetings/skype meetings etc are not appearing in my outlook calendar, can somebody please advise how to correct this?\\r\\n\\r\\nkind',\n",
    "           'Caller230 . Latin . skype error',\n",
    "           'Caller1871 . English . unable to login to hr_tool to sgxqsuojr xwbesorf cards',\n",
    "           ...,\n",
    "           \"Caller424 . English . oneteam sso not working . i'm unable to log in to hr_tool/oneteam through the sso portal on the hub. whenever i click on the oneteam link in the portal i'm taken to a login screen instead of being logged directly into the system. i have tried clearing my browser cache and i've tried logging in using both ie and firefox.\",\n",
    "           \"Caller424 . English . oneteam sso not working . i'm unable to log in to hr_tool/oneteam through the sso portal on the hub. whenever i click on the oneteam link in the portal i'm taken to a login screen instead of being logged directly into the system. i have tried clearing my browser cache and i've tried logging in using both ie and firefox.\",\n",
    "           \"Caller424 . English . oneteam sso not working . i'm unable to log in to hr_tool/oneteam through the sso portal on the hub. whenever i click on the oneteam link in the portal i'm taken to a login screen instead of being logged directly into the system. i have tried clearing my browser cache and i've tried logging in using both ie and firefox.\"],\n",
    "          dtype=object)\n",
    "\n",
    "# BERT - Define Bert Tokenizer, Model def<a href=\"#BERT---Define-Bert-Tokenizer,-Model-def\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "Prep input data tensors, define BERT model for multi class\n",
    "classification useing pre-trained model from Huggingface BERT\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', \n",
    "                                              do_lower_case=True)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type=='train'].XCombined.values, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        pad_to_max_length=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(\n",
    "        df[df.data_type=='val'].XCombined.values, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        pad_to_max_length=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "    labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "    Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
    "    Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'only_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you may want to check this is the right behavior.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    len(dataset_train), len(dataset_val)\n",
    "\n",
    "Out\\[ \\]:\n",
    "\n",
    "    (19282, 3403)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "    Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
    "    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
    "    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
    "    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "    batch_size = 3\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, \n",
    "                                  sampler=RandomSampler(dataset_train), \n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "    dataloader_validation = DataLoader(dataset_val, \n",
    "                                       sampler=SequentialSampler(dataset_val), \n",
    "                                       batch_size=batch_size)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=1e-5, \n",
    "                      eps=1e-8)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    epochs = 5\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    def accuracy(preds, labels):\n",
    "        outputs = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(outputs == labels_flat)\n",
    "\n",
    "    def f1_score_func(preds, labels):\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "    def accuracy_per_class(preds, labels):\n",
    "        label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "        \n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "\n",
    "        for label in np.unique(labels_flat):\n",
    "            y_preds = preds_flat[labels_flat==label]\n",
    "            y_true = labels_flat[labels_flat==label]\n",
    "            print(f'Class: {label_dict_inverse[label]}')\n",
    "            print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    import random\n",
    "\n",
    "    seed_val = 17\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    print(device)\n",
    "\n",
    "    cuda\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    def evaluate(dataloader_val):\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        loss_val_total = 0\n",
    "        predictions, true_vals = [], []\n",
    "        total_eval_accuracy = 0\n",
    "        \n",
    "        for batch in dataloader_val:\n",
    "            \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                     }\n",
    "\n",
    "            with torch.no_grad():        \n",
    "                outputs = model(**inputs)\n",
    "                \n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss_val_total += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = inputs['labels'].cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_vals = np.concatenate(true_vals, axis=0)\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(dataloader_val)\n",
    "      \n",
    "                \n",
    "        return loss_val_avg, predictions, true_vals, avg_val_accuracy\n",
    "                \n",
    "      \n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "        #import torch\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "# OPTIONAL - CHECKPOINT RESTART SECTION -RUN ONLY NEEDED<a href=\"#OPTIONAL---CHECKPOINT-RESTART-SECTION--RUN-ONLY-NEEDED\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    epochs = 4\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load('bbmc_finetuned_BERT_epoch_C.model', map_location=torch.device('cpu')))\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    print(device)\n",
    "    #epochs = 5\n",
    "\n",
    "    Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
    "    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
    "    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
    "    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "    cuda\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    epochs = 2\n",
    "\n",
    "# BERT Training<a href=\"#BERT-Training\" class=\"anchor-link\">¶</a>\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                     }       \n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "             \n",
    "            \n",
    "        #torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "        torch.save(model.state_dict(), 'bbmc_finetuned_BERT_epoch_{}.model'.format(epoch))\n",
    "\n",
    "            \n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        \n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        \n",
    "        val_loss, predictions, true_vals, avg_val_accuracy = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "    Epoch 1\n",
    "    Training loss: 0.8938806672964776\n",
    "    Validation loss: 0.8860058817864778\n",
    "    F1 Score (Weighted): 0.777262280349045\n",
    "    Accuracy: 0.8038179148311304\n",
    "\n",
    "    Epoch 2\n",
    "    Training loss: 0.8940092379896071\n",
    "    Validation loss: 0.8860058817864778\n",
    "    F1 Score (Weighted): 0.777262280349045\n",
    "    Accuracy: 0.8038179148311304\n",
    "\n",
    "# OPTIONAL - Run only if GPU memory error above section. Manual Training<a\n",
    "href=\"#OPTIONAL---Run-only-if-GPU-memory--error-above-section.-Manual-Training\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "Run outside the epochs iteration only if needed\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "        val_loss, predictions, true_vals, avg_val_accuracy = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "    Validation loss: 0.9904041105922361\n",
    "    F1 Score (Weighted): 0.7650140247295772\n",
    "    Accuracy: 0.7964757709251095\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # upload external file before import\n",
    "    #from google.colab import files\n",
    "    #files.upload()\n",
    "    #import helper\n",
    "\n",
    "    #files.upload()\n",
    "    #import fc_model\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #epoch 1 \n",
    "    #torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    # download checkpoint file\n",
    "    #files.download('checkpoint.pth')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #torch.save(model.state_dict(), 'finetuned_BERT_epoch_{epoch}.model')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "        \n",
    "        torch.save(model.state_dict(), 'bbmc_finetuned_BERT_epoch_{epoch1}.model')\n",
    "            \n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        \n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        \n",
    "        val_loss, predictions, true_vals, avg_val_accuracy  = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "    Epoch 1\n",
    "    Training loss: 0.4689319437316262\n",
    "    Validation loss: 0.07559893933844199\n",
    "    F1 Score (Weighted): 0.9885210834945934\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    # epoch 2 manual run\n",
    "    for epoch in tqdm(range(2, 3)):\n",
    "        model.train()\n",
    "        \n",
    "        loss_train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "        for batch in progress_bar:\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[2],\n",
    "                     }       \n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "             \n",
    "            \n",
    "        #torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "        torch.save(model.state_dict(), 'bbmc_finetuned_BERT_epoch_{}.model'.format(epoch))\n",
    "\n",
    "            \n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        \n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        \n",
    "        val_loss, predictions, true_vals, avg_val_accuracy  = evaluate(dataloader_validation)\n",
    "        val_f1 = f1_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'F1 Score (Weighted): {val_f1}')\n",
    "        tqdm.write(f'Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    #epoch 2 \n",
    "    torch.save(model.state_dict(), 'checkpoint2.pth')\n",
    "\n",
    "In \\[ \\]:\n",
    "\n",
    "    epoch = 2\n",
    "    print('finetuned_BERT_epoch_{}.model'.format(epoch))\n",
    "\n",
    "    finetuned_BERT_epoch_2.model\n",
    "\n",
    "# Load the previously SAVED MODEL (Choose file here)<a href=\"#Load-the-previously-SAVED-MODEL-(Choose-file-here)\"\n",
    "class=\"anchor-link\">¶</a>\n",
    "\n",
    "file for new prediction/ evaluation\n",
    "\n",
    "In \\[48\\]:\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\",\n",
    "                                                          num_labels=len(label_dict),\n",
    "                                                          output_attentions=False,\n",
    "                                                          output_hidden_states=False)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
    "    - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
    "    - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
    "    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "Out\\[48\\]:\n",
    "\n",
    "    BertForSequenceClassification(\n",
    "      (bert): BertModel(\n",
    "        (embeddings): BertEmbeddings(\n",
    "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
    "          (position_embeddings): Embedding(512, 768)\n",
    "          (token_type_embeddings): Embedding(2, 768)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (encoder): BertEncoder(\n",
    "          (layer): ModuleList(\n",
    "            (0): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (1): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (2): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (3): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (4): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (5): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (6): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (7): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (8): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (9): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (10): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "            (11): BertLayer(\n",
    "              (attention): BertAttention(\n",
    "                (self): BertSelfAttention(\n",
    "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "                (output): BertSelfOutput(\n",
    "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                  (dropout): Dropout(p=0.1, inplace=False)\n",
    "                )\n",
    "              )\n",
    "              (intermediate): BertIntermediate(\n",
    "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "              )\n",
    "              (output): BertOutput(\n",
    "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "                (dropout): Dropout(p=0.1, inplace=False)\n",
    "              )\n",
    "            )\n",
    "          )\n",
    "        )\n",
    "        (pooler): BertPooler(\n",
    "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "          (activation): Tanh()\n",
    "        )\n",
    "      )\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "      (classifier): Linear(in_features=768, out_features=74, bias=True)\n",
    "    )\n",
    "\n",
    "In \\[49\\]:\n",
    "\n",
    "    #model.load_state_dict(torch.load('data_volume/finetuned_BERT_epoch_1.model', map_location=torch.device('cpu')))\n",
    "    model.load_state_dict(torch.load('bbmc_finetuned_BERT_epoch_2.model', map_location=torch.device('cpu')))\n",
    "\n",
    "Out\\[49\\]:\n",
    "\n",
    "    <All keys matched successfully>\n",
    "\n",
    "In \\[50\\]:\n",
    "\n",
    "    avg_val_loss, predictions, true_vals, avg_val_accuracy  = evaluate(dataloader_validation)\n",
    "\n",
    "In \\[51\\]:\n",
    "\n",
    "    accuracy_per_class(predictions, true_vals)\n",
    "\n",
    "    Class: GRP_0\n",
    "    Accuracy: 565/596\n",
    "\n",
    "    Class: GRP_8\n",
    "    Accuracy: 72/99\n",
    "\n",
    "    Class: GRP_9\n",
    "    Accuracy: 17/38\n",
    "\n",
    "    Class: GRP_12\n",
    "    Accuracy: 3/39\n",
    "\n",
    "    Class: GRP_24\n",
    "    Accuracy: 41/43\n",
    "\n",
    "    Class: GRP_1\n",
    "    Accuracy: 27/37\n",
    "\n",
    "    Class: GRP_3\n",
    "    Accuracy: 0/38\n",
    "\n",
    "    Class: GRP_4\n",
    "    Accuracy: 6/38\n",
    "\n",
    "    Class: GRP_5\n",
    "    Accuracy: 12/38\n",
    "\n",
    "    Class: GRP_6\n",
    "    Accuracy: 0/37\n",
    "\n",
    "    Class: GRP_7\n",
    "    Accuracy: 33/37\n",
    "\n",
    "    Class: GRP_10\n",
    "    Accuracy: 14/38\n",
    "\n",
    "    Class: GRP_11\n",
    "    Accuracy: 30/37\n",
    "\n",
    "    Class: GRP_13\n",
    "    Accuracy: 26/38\n",
    "\n",
    "    Class: GRP_14\n",
    "    Accuracy: 17/37\n",
    "\n",
    "    Class: GRP_15\n",
    "    Accuracy: 26/37\n",
    "\n",
    "    Class: GRP_16\n",
    "    Accuracy: 25/38\n",
    "\n",
    "    Class: GRP_17\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_18\n",
    "    Accuracy: 28/38\n",
    "\n",
    "    Class: GRP_19\n",
    "    Accuracy: 3/38\n",
    "\n",
    "    Class: GRP_2\n",
    "    Accuracy: 10/38\n",
    "\n",
    "    Class: GRP_20\n",
    "    Accuracy: 28/37\n",
    "\n",
    "    Class: GRP_21\n",
    "    Accuracy: 36/38\n",
    "\n",
    "    Class: GRP_22\n",
    "    Accuracy: 30/37\n",
    "\n",
    "    Class: GRP_23\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_25\n",
    "    Accuracy: 23/38\n",
    "\n",
    "    Class: GRP_26\n",
    "    Accuracy: 13/38\n",
    "\n",
    "    Class: GRP_27\n",
    "    Accuracy: 36/37\n",
    "\n",
    "    Class: GRP_28\n",
    "    Accuracy: 16/38\n",
    "\n",
    "    Class: GRP_29\n",
    "    Accuracy: 19/38\n",
    "\n",
    "    Class: GRP_30\n",
    "    Accuracy: 28/37\n",
    "\n",
    "    Class: GRP_31\n",
    "    Accuracy: 0/38\n",
    "\n",
    "    Class: GRP_33\n",
    "    Accuracy: 22/38\n",
    "\n",
    "    Class: GRP_34\n",
    "    Accuracy: 7/37\n",
    "\n",
    "    Class: GRP_35\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_36\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_37\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_38\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_39\n",
    "    Accuracy: 35/37\n",
    "\n",
    "    Class: GRP_40\n",
    "    Accuracy: 27/37\n",
    "\n",
    "    Class: GRP_41\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_42\n",
    "    Accuracy: 30/38\n",
    "\n",
    "    Class: GRP_43\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_44\n",
    "    Accuracy: 31/37\n",
    "\n",
    "    Class: GRP_45\n",
    "    Accuracy: 27/37\n",
    "\n",
    "    Class: GRP_46\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_47\n",
    "    Accuracy: 35/38\n",
    "\n",
    "    Class: GRP_48\n",
    "    Accuracy: 35/37\n",
    "\n",
    "    Class: GRP_49\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_50\n",
    "    Accuracy: 30/37\n",
    "\n",
    "    Class: GRP_51\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_52\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_53\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_54\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_55\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_56\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_57\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_58\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_59\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_60\n",
    "    Accuracy: 36/38\n",
    "\n",
    "    Class: GRP_61\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_32\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_62\n",
    "    Accuracy: 36/37\n",
    "\n",
    "    Class: GRP_63\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_64\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_65\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_66\n",
    "    Accuracy: 37/37\n",
    "\n",
    "    Class: GRP_67\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_68\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_69\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_70\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_71\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_72\n",
    "    Accuracy: 38/38\n",
    "\n",
    "    Class: GRP_73\n",
    "    Accuracy: 37/37\n",
    "\n",
    "In \\[52\\]:\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "      Accuracy: 0.80\n",
    "      Validation Loss: 0.89"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
